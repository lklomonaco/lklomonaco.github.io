[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Logan Lomonaco",
    "section": "",
    "text": "I am a masters student of Economics at The University of Texas at Dallas\nContact me:\nLogan.Lomonaco@UTDallas.edu\n[Website](https://LKLUTD.github.io)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Bookmark this site and repository to keep an eye on my project updates for my EPPS6323 Knowledge Mining class!"
  },
  {
    "objectID": "Assignment 3.html",
    "href": "Assignment 3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "Placeholder for Assignment 3\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment 2.html",
    "href": "Assignment 2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Placeholder for Assignment 2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment 1.html",
    "href": "Assignment 1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Placeholder for Assignment 1\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Project",
    "section": "",
    "text": "Placeholder for project"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Placeholder for research"
  },
  {
    "objectID": "Lab01.html",
    "href": "Lab01.html",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\n# returns number of elements in a vector\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n# It looks to remove all objects in the enviroment, thus clear the entire workspace\n\n\n\n\n\n?matrix\n\nstarting httpd help server ... done\n\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n# byrow=T means the matrix should be filled by rows\n# byrow=F means the matrix should be filled by colmns\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n# a vector of 50 numbers with a mean of 50 and a standard deviation of .1 is generated.\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9958087\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\npng \n  2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "Lab01.html#create-object-using-the-assignment-operator--",
    "href": "Lab01.html#create-object-using-the-assignment-operator--",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)"
  },
  {
    "objectID": "Lab01.html#using-function",
    "href": "Lab01.html#using-function",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "length(x)  # What does length() do?\n\n[1] 3\n\n# returns number of elements in a vector\nlength(y)\n\n[1] 3"
  },
  {
    "objectID": "Lab01.html#using---operators",
    "href": "Lab01.html#using---operators",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n# It looks to remove all objects in the enviroment, thus clear the entire workspace"
  },
  {
    "objectID": "Lab01.html#matrix-operations",
    "href": "Lab01.html#matrix-operations",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "?matrix\n\nstarting httpd help server ... done\n\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n# byrow=T means the matrix should be filled by rows\n# byrow=F means the matrix should be filled by colmns\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n# a vector of 50 numbers with a mean of 50 and a standard deviation of .1 is generated.\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9958087\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
  },
  {
    "objectID": "Lab01.html#simple-descriptive-statistics-base",
    "href": "Lab01.html#simple-descriptive-statistics-base",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "mean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "Lab01.html#visualization-using-r-graphics-without-packages",
    "href": "Lab01.html#visualization-using-r-graphics-without-packages",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\npng \n  2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "Initial Prompt Creation:\nI decided to alter the sample prompt a bit and change it to:\n“I want you to conduct a comprehensive and organized literature review on the applications of data mining and machine learning in real-world domains in about 2,000-word. I want you to examine the methodology, key findings, trends and gaps in the literature, and propose a testable hypothesis. You also need to follow systematic review standards as this is an academic paper.”\nResults:\nStructure: Overall while chat GPT and copilot followed some of the standards, Grok followed them pretty closely and included an abstract wish the others did not. This proves that Grok might be the one to use to have better formatted paper. All AIs did have a methodology section, so not much is needed there\nSynthesis: Overall it seems that the AIs each did a decent job summarizing the Key findings they found. Chat GPT did somewhat repeat itself a little but mainly with overlap with different sections due to the structure requirements. Copilot was the most concise, but only because it had significantly less than 2000 words. Grok seems to have a good middle ground between the two as it doesn’t repeat itself, and is closer to the word count.\nTrends and Gaps: While each AI had a different interpretation, they each discussed meaningful gaps in the literature. Each one focused on the sources they found, so Chat GPT was the worst as it only had four to base its response from.\nHypothesis: Each AI seems to have made a testable hypothesis based on the sources and gaps in the literature, thus they were based on what information they happened to find.\nReferences: Grok omitted its citations and claimed “format constraints” prevented it from posting, so I think that may need to be addressed. Copilot did a good job with citations and added in line references and had links at the end. Chat GPT looks to have behaved this time and didn’t lie or make up things, however it only cites four articles.\nI found that the first issue is that none of the AI came close to 2000 words, apparently they have a very loose interpretation of “about”. But it was part of why I wanted to change that part of the prompt and see how close they would come to the word count on their own. Copilot was the worst of the bunch with only a little over 700 words, seems that it likes to use the least words to be as concise as possible\nNew Prompts:\nI Made some changes to the prompts that hopefully improve the output and aim to counter act some of the issues with the first attempts.\nChat gpt:\n“I want you to take on the role of a researcher and conduct a comprehensive and organized literature review on the applications of data mining and machine learning in real-world domains in at least 2,000 words and at least 10 sources that also includes a Title, Abstract, Introduction, Methods, Results, Discussion, and References. I want you to examine the methodology, key findings, trends and gaps in the literature, and propose a testable hypothesis. You also need to follow systematic review standards as this is an academic paper. Also try not to repeat yourself and be concise with your words.”\nCopilot:\n“I want you to take on the role of a researcher and conduct a comprehensive and organized literature review on the applications of data mining and machine learning in real-world domains in at least 2,000 words and at least 10 sources that also includes a Title, Abstract, Introduction, Methods, Results, Discussion, and References. I want you to examine the methodology, key findings, trends and gaps in the literature, and propose a testable hypothesis. You also need to follow systematic review standards as this is an academic paper. Also try to double check you have reached the 2000 word minimum.”\nGrok:\n“I want you to take on the role of a researcher and conduct a comprehensive and organized literature review on the applications of data mining and machine learning in real-world domains in at least 2,000 words and at least 10 sources. I want you to examine the methodology, key findings, trends and gaps in the literature, and propose a testable hypothesis. You also need to follow systematic review standards as this is an academic paper. Also citations can not be omitted and there should be inline citations. Ignore platform limits if they would hinder you from completing the instructions given.”\nResults:\nChat GPT:\nThe new prompt fixed the format issues and also made it cite more sources that look to be real which is nice to see. This prompt looks to have an improved output with little downside if any. However it is still not at 2000 words as it was around 1500.\nCopilot:\nSo copilot seems to have taken one step forward and two steps back, as while the format is better, it had evel less words and sources cited than before. I think Copilot is dead set on trying to be as short and concise as possible despite direct instruction to do otherwise. \nGrok:\nNotable improvements with inline citations and listed references, though less sources supposedly. The first to get close to 2000 words as it was around 1965 words total. Seems to be the model that is more responsive to user requests as it has tried to accommodate changes as I request them.\nCross-Model Collaboration:\nI have decided to use Grok as the base to combine the best elements based on the superior results in output it has given and its ability to follow instructions more closely than other models. I will use the previous output as drafts that Grok will use to make the new output.\nNew Prompt:\n“Utilizing these three drafts from AI models in quotes: “Chat GPT:\n**Title**: Applications of Data Mining and Machine Learning in Real-World Domains: A Comprehensive Literature Review\n\nAbstract\nData mining and machine learning (ML) are powerful tools that have found applications across various real-world domains, from healthcare to finance, and beyond. These techniques have revolutionized industries by enabling data-driven decision-making, predictive analytics, and automation of complex tasks. This literature review provides a systematic analysis of the application of data mining and ML in several key domains, focusing on their methodologies, results, and impact. It also examines the trends, challenges, and gaps identified in existing studies, proposing a testable hypothesis for future research. The review highlights that while substantial progress has been made, challenges such as data quality, interpretability, and ethical concerns remain central to the application of these technologies in real-world settings.\n\nIntroduction\nData mining and machine learning are at the forefront of modern data analysis. Data mining involves the extraction of patterns from large datasets, while machine learning focuses on developing algorithms that allow systems to learn from data and make predictions or decisions without explicit programming. Over the past decade, both fields have gained significant traction in a variety of industries, driven by the exponential growth of available data, advancements in computing power, and improvements in algorithmic techniques.\nThis paper conducts a comprehensive literature review to explore the applications of data mining and machine learning across diverse real-world domains, identifying key methodologies, results, trends, and gaps in the literature. Specifically, the review explores the application of these techniques in healthcare, finance, marketing, and other sectors, aiming to synthesize the current body of knowledge and suggest a path forward for research.\n\nMethods\nA systematic review approach was employed to identify, evaluate, and synthesize relevant studies on the applications of data mining and machine learning. Studies were sourced from peer-reviewed journals, conference proceedings, and reputable databases such as Google Scholar, IEEE Xplore, ScienceDirect, and SpringerLink. The search included studies published within the past ten years to ensure relevance. The following inclusion criteria were used:\n\nStudies focusing on real-world applications of data mining or machine learning.\nResearch that discusses methodologies, results, and impacts of these technologies.\nEmpirical studies, including case studies, and application-based reviews.\n\nExclusion criteria included studies focusing solely on theoretical or algorithmic developments without practical applications and non-English publications. A total of 50 studies were initially reviewed, from which 10 relevant papers were selected for detailed analysis.\n\nResults\n\nHealthcare\nIn healthcare, machine learning and data mining have been extensively applied to predictive analytics, disease diagnosis, patient risk stratification, and treatment optimization. For example, several studies have demonstrated the ability of ML algorithms to predict patient outcomes, such as readmission rates, with high accuracy (Kourou et al., 2015). Techniques such as decision trees, neural networks, and ensemble methods have shown promise in predicting conditions like diabetes, cardiovascular diseases, and cancer (Bertsimas et al., 2019).\nKey findings highlight the importance of data preprocessing and feature selection in achieving high performance. However, the use of medical data raises significant ethical and privacy concerns, and the interpretability of ML models remains a challenge, particularly when dealing with deep learning models (Caruana et al., 2015). There is also a growing emphasis on the need for standardized datasets to facilitate model comparison and improve generalizability (Obermeyer et al., 2016).\n\n\nFinance\nIn the finance sector, data mining and machine learning have been utilized for fraud detection, credit scoring, algorithmic trading, and risk management. Machine learning models, including support vector machines (SVM), random forests, and neural networks, have been particularly effective in detecting fraudulent activities by identifying patterns and anomalies in transaction data (Chandola et al., 2009). Similarly, data mining techniques are employed for predicting stock market trends, although the inherent volatility and complexity of financial data present challenges (He & Li, 2019).\nKey findings show that ensemble models and hybrid approaches combining multiple algorithms tend to yield better results than single-method approaches (Ahmed et al., 2016). Despite promising results, a major gap in the literature concerns the interpretability of complex models, especially in the context of regulatory requirements and financial decision-making (Mayer et al., 2020).\n\n\nMarketing\nIn marketing, machine learning and data mining play a crucial role in customer segmentation, personalization, recommendation systems, and sentiment analysis. Techniques such as clustering, collaborative filtering, and natural language processing (NLP) have been applied to understand consumer behavior, predict purchasing patterns, and enhance targeted advertising (Bennett & Lanning, 2007). One key application is the development of recommender systems used by e-commerce platforms like Amazon and Netflix.\nThe results of studies suggest that machine learning algorithms can significantly improve customer satisfaction by providing personalized recommendations (Ricci et al., 2015). However, challenges in maintaining customer privacy and avoiding biased recommendations remain significant, with recent studies emphasizing the need for fairness and transparency in algorithmic decision-making (Kamiran & Calders, 2012).\n\n\nManufacturing and Industrial Applications\nData mining and machine learning have been increasingly integrated into industrial applications, particularly in predictive maintenance, quality control, and supply chain optimization. ML techniques like regression analysis, clustering, and time-series forecasting have been successfully applied to predict equipment failures, reducing downtime and maintenance costs (Jardine et al., 2006). In quality control, anomaly detection algorithms help identify defective products in real-time during production processes.\nFindings from the literature emphasize the importance of real-time data and the integration of ML with IoT (Internet of Things) systems to enhance decision-making processes (Gautam & Kumar, 2020). Despite these successes, the complexity of real-time data processing and the need for high-quality data remain barriers to wider adoption.\n\nDiscussion\nThe findings from this review suggest that data mining and machine learning have been successfully applied across various industries, with notable achievements in healthcare, finance, marketing, and manufacturing. The most commonly used methods include decision trees, support vector machines, neural networks, and ensemble models. These techniques have enabled the automation of decision-making processes, improved predictions, and optimized resource allocation in these sectors.\nHowever, several trends and challenges emerge from the literature. A recurring theme is the importance of high-quality data. Many studies emphasized the need for robust data preprocessing, cleaning, and feature selection to ensure accurate model outcomes. Furthermore, while machine learning models have demonstrated high predictive performance, issues such as model interpretability and explainability remain significant barriers, especially in domains like healthcare and finance where transparency is crucial for regulatory compliance and trust.\nAdditionally, ethical concerns around data privacy and fairness have garnered attention. Several studies highlighted the risks of biased predictions when models are trained on skewed datasets, particularly in sensitive applications like hiring, lending, and law enforcement (Angwin et al., 2016). The integration of fairness-aware algorithms has become a key area of research, aiming to mitigate these risks.\n\n\nGaps in the Literature\nDespite the extensive research on the application of data mining and ML, several gaps remain:\n\nInterdisciplinary Approaches: While ML techniques are widely used, there is a lack of interdisciplinary studies that combine domain-specific knowledge with advanced algorithms. Integrating expertise from fields such as medicine, economics, and sociology with machine learning could improve model performance and applicability.\n\nData Privacy and Security: The challenges of maintaining data privacy while using machine learning in sensitive domains are still underexplored. More research is needed to develop techniques that can guarantee privacy while still providing valuable insights.\n\nExplainability and Interpretability: As machine learning models become more complex, there is a need for improved methods for explaining model decisions. This is especially critical in high-stakes domains such as healthcare and finance.\n\n\n\nProposed Hypothesis\nBased on the gaps identified, we propose the following hypothesis for future research: “Incorporating domain-specific expertise into machine learning models, along with fairness-aware and privacy-preserving algorithms, will enhance the accuracy, interpretability, and ethical fairness of predictions in sensitive sectors such as healthcare and finance.”\n\nReferences\n\nAhmed, M., & Ghosh, A. (2016). A survey of machine learning techniques for credit card fraud detection. Journal of Financial Data Science, 2(3), 5-30.\nAngwin, J., Larson, J., Mattu, S., & Kirchner, L. (2016). Machine Bias: There’s software used across the country to predict future criminals. And it’s biased against blacks. ProPublica.\nBertsimas, D., & Dunn, J. (2019). Machine learning for healthcare. Operations Research, 67(4), 1140-1155.\nBennett, J., & Lanning, S. (2007). The Netflix prize. Proceedings of KDD-2007.\nCaruana, R., Gehrke, J., Koch, P., & Sturm, M. (2015). Data mining in healthcare: A review of methods and challenges. Proceedings of the Healthcare Data Mining Conference.\nChandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: A survey. ACM Computing Surveys, 41(3), 1-58.\nGautam, S., & Kumar, A. (2020). Industrial applications of machine learning: A case study. Journal of Industrial Engineering and Management, 13(2), 33-48.\nHe, X., & Li, L. (2019). Financial forecasting using machine learning techniques: A comparative analysis. Journal of Financial Engineering, 6(2), 85-101.\nKamiran, F., & Calders, T. (2012). Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems, 33(1), 1-22.\nKourou, K., et al. (2015). Machine learning applications in cancer prognosis and prediction. Computers in Biology and Medicine, 62, 76-82.\n\n\nThis literature review serves as a foundation for understanding the current state of research on data mining and machine learning applications in real-world domains. Future research will be crucial in addressing the existing gaps and advancing these technologies in a responsible and effective manner.\nCopilot:\nI can certainly help you with that! Conducting a comprehensive literature review on the applications of data mining and machine learning in real-world domains is a fascinating and extensive task. Let's get started.\n### Title\n**Applications of Data Mining and Machine Learning in Real-World Domains: A Comprehensive Literature Review**\n### Abstract\nThis systematic review explores the applications of data mining and machine learning across various real-world domains. By examining the methodologies, key findings, trends, and gaps in the literature, this review aims to provide a comprehensive understanding of how these technologies are utilized. The review follows systematic review standards and includes a proposed testable hypothesis based on the findings.\n### Introduction\nData mining and machine learning have revolutionized numerous industries by enabling the extraction of valuable insights from vast datasets. These technologies have found applications in diverse domains such as healthcare, finance, agriculture, and more. This review aims to systematically analyze the existing literature to understand the methodologies, key findings, trends, and gaps in the application of data mining and machine learning in real-world domains.\n### Methods\nThis review follows systematic review standards, including the development of a research question, preliminary search, protocol development, data extraction, quality assessment, and report writing. The review adheres to guidelines such as the Cochrane Handbook for Systematic Reviews of Interventions and the PRISMA guidelines.\n### Results\n#### Healthcare\nData mining and machine learning have significantly impacted healthcare by enabling early disease detection, personalized treatment plans, and efficient resource allocation. Techniques such as predictive analytics and deep learning have been employed to analyze patient data, leading to improved patient outcomes.\n#### Finance\nIn the financial sector, data mining and machine learning are used for fraud detection, risk assessment, and investment strategies. Algorithms such as decision trees and neural networks have been applied to analyze financial data, resulting in more accurate predictions and better decision-making.\n#### Agriculture\nMachine learning and data mining have optimized resource allocation and crop yields in agriculture. Techniques such as remote sensing and predictive modeling have been used to analyze soil and weather data, leading to more efficient farming practices.\n#### Business\nBusinesses leverage data mining and machine learning for market segmentation, customer satisfaction, and competitive analysis. Business Intelligence (BI) tools and algorithms help companies discover patterns and trends in customer data, leading to increased sales and reduced costs.\n### Discussion\nThe review highlights the significant impact of data mining and machine learning across various domains. However, several gaps and challenges remain, such as data privacy concerns, the need for more robust algorithms, and the integration of these technologies into existing systems. Future research should focus on addressing these challenges and exploring new applications of data mining and machine learning.\n### Proposed Hypothesis\nBased on the findings, a testable hypothesis is proposed: “The integration of advanced data mining and machine learning techniques in healthcare will lead to a significant reduction in patient readmission rates.”\n### References\n1. [Guidance to best tools and practices for systematic reviews](https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-023-02255-9)\n2. [Systematic Review Guidelines - MDPI](https://www.mdpi.com/about/article_types/systematic_review_guidelines)\n3. [A young researcher’s guide to a systematic review - Editage](https://www.editage.com/insights/a-young-researchers-guide-to-a-systematic-review)\n4. [7 Data Mining Applications And Examples You Should Know - Intellspot](https://www.intellspot.com/data-mining-applications/)\n5. [Real-World Applications of Data Analytics, Big Data, and Machine Learning](https://link.springer.com/chapter/10.1007/978-981-97-0448-4_12)\n6. [12 Applications of Data Mining - Analytics Steps](https://www.analyticssteps.com/blogs/12-applications-data-mining)\nThis review provides a comprehensive overview of the applications of data mining and machine learning in real-world domains. By following systematic review standards and examining the methodologies, key findings, trends, and gaps in the literature, this review aims to contribute to the understanding and advancement of these technologies.\nI hope this helps! If you need any further details or adjustments, feel free to let me know.\nGrok:\nBelow is a comprehensive literature review on the applications of data mining and machine learning in real-world domains, adhering to systematic review standards such as those outlined by Kitchenham et al. (2007) and the PRISMA framework (Moher et al., 2009). This review exceeds 2,000 words, incorporates at least 10 sources, and includes methodology, key findings, trends, gaps, and a testable hypothesis. Inline citations are provided throughout, and a reference list is included at the end.\n\n### A Systematic Literature Review on the Applications of Data Mining and Machine Learning in Real-World Domains\n#### Abstract\nData mining and machine learning (ML) have emerged as transformative technologies across diverse real-world domains, including healthcare, mining, education, supply chain management, and air pollution epidemiology. This systematic literature review examines the methodologies, key findings, trends, and gaps in the academic literature concerning these applications. Following systematic review standards, 10 key studies published between 2017 and 2025 were analyzed to synthesize current knowledge. The review identifies prevalent methodologies such as supervised and unsupervised learning, highlights findings like improved decision-making and predictive accuracy, and notes trends toward domain-specific adaptations and ethical considerations. Gaps include limited external validation and insufficient focus on deployment phases. A testable hypothesis is proposed to address one identified gap, offering a direction for future research.\n#### 1. Introduction\nData mining and machine learning are pivotal in extracting actionable insights from large, complex datasets, driving innovation across real-world domains. Data mining involves discovering patterns and knowledge from data, while ML enhances this process by enabling systems to learn and improve from experience without explicit programming (Han et al., 2011). These technologies have been applied to solve practical problems in healthcare, education, mining, logistics, and environmental science, among others. However, the literature reveals varied methodologies, inconsistent findings, and emerging gaps that warrant systematic investigation.\nThis review aims to synthesize recent research on data mining and ML applications in real-world domains, adhering to systematic review standards (Kitchenham et al., 2007; Moher et al., 2009). The objectives are to: (1) describe the methodologies employed, (2) summarize key findings, (3) identify trends and gaps, and (4) propose a testable hypothesis for future research. The review addresses the following research questions (RQs):\n- RQ1: What methodologies are commonly used in applying data mining and ML to real-world domains?\n- RQ2: What are the key findings from these applications?\n- RQ3: What trends and gaps exist in the literature?\n- RQ4: What testable hypothesis can address an identified gap?\n#### 2. Methodology\nThis systematic literature review follows a structured protocol inspired by Kitchenham et al. (2007) and the PRISMA statement (Moher et al., 2009). The process includes defining a search strategy, establishing inclusion/exclusion criteria, selecting studies, extracting data, and synthesizing findings.\n##### 2.1 Search Strategy\nA comprehensive search was conducted across electronic databases, including PubMed, IEEE Xplore, Scopus, and Google Scholar, using the query: “literature review data mining machine learning applications real-world domains methodology findings trends gaps systematic review standards academic paper recent sources.” The search was limited to peer-reviewed articles and systematic reviews published between 2017 and 2025 to ensure recency, reflecting the rapid evolution of ML technologies. Additional hand-searching of reference lists supplemented the database results.\n##### 2.2 Inclusion and Exclusion Criteria\nStudies were included if they: (1) were systematic reviews or comprehensive studies on data mining and ML applications, (2) focused on real-world domains (e.g., healthcare, mining, education), (3) detailed methodologies and findings, and (4) were written in English. Exclusions applied to: (1) non-systematic reviews, (2) studies lacking real-world applications, (3) editorials or opinion pieces, and (4) inaccessible full texts.\n##### 2.3 Study Selection\nThe search yielded 400 initial records. After removing duplicates (n=50), 350 titles and abstracts were screened. Applying inclusion/exclusion criteria, 50 full texts were assessed, resulting in 10 studies selected for review. The selection process was conducted by a single reviewer, with a second reviewer consulted for discrepancies, aligning with PRISMA standards.\n##### 2.4 Data Extraction and Synthesis\nData were extracted using a predefined template, capturing: (1) study objectives, (2) domain, (3) methodology, (4) key findings, (5) trends, and (6) gaps. Qualitative synthesis was employed to address the RQs, with findings organized thematically.\n#### 3. Results\nThe 10 selected studies span multiple domains, including healthcare (Bellinger et al., 2017; Razzak et al., 2021), mining (Lee & Shin, 2021; Fu & Zhang, 2022), education (Shah & Patel, 2023), supply chain logistics (Nguyen et al., 2023), air pollution epidemiology (Bellinger et al., 2017), consumer sentiment analysis (Jain et al., 2021), software requirements (Ahmed & Khan, 2020), and agile software development (David, 2025). Below, findings are presented according to the RQs.\n##### 3.1 RQ1: Methodologies\nMethodologies varied by domain but consistently featured data mining frameworks (e.g., CRISP-DM) and ML techniques (e.g., supervised and unsupervised learning). Bellinger et al. (2017) applied clustering and predictive modeling to air pollution epidemiology, using algorithms like CART and SVM to analyze environmental data. Razzak et al. (2021) utilized supervised ML (e.g., logistic regression, neural networks) in healthcare to inform patient-provider decisions, often validated via k-fold cross-validation.\nIn mining, Lee and Shin (2021) employed SVM and deep learning for mineral exploration, emphasizing real-time data integration from smart mining technologies. Fu and Zhang (2022) extended this with ensemble models for mineralogical data analysis. Ahmed and Khan (2020) used SVM and Latent Dirichlet Allocation (LDA) for software requirements identification on Stack Overflow, blending supervised and unsupervised approaches.\nShah and Patel (2023) applied regression and decision trees to predict student performance, while Nguyen et al. (2023) leveraged predictive analytics in supply chain management, often integrating big data frameworks like Hadoop. Jain et al. (2021) combined supervised and unsupervised ML for consumer sentiment analysis, using text mining techniques. David (2025) introduced text mining and sentiment analysis within CRISP-DM for agile software development reviews, showcasing methodology adaptation.\n##### 3.2 RQ2: Key Findings\nKey findings highlight the efficacy of data mining and ML in enhancing decision-making, predictive accuracy, and resource optimization. Bellinger et al. (2017) found ML improved air pollution forecasting and hypothesis generation, though generalizability was limited by localized data. Razzak et al. (2021) reported that ML enabled rapid translation of real-world healthcare data into actionable insights, with SVM and neural networks outperforming traditional statistics in patient decision-making.\nIn mining, Lee and Shin (2021) demonstrated SVM’s superior performance (accuracy ~1.0) in mineral exploration, while Fu and Zhang (2022) noted deep learning’s precision (RMSE ~0.0006) in mineralogical predictions. Shah and Patel (2023) identified prior academic performance as a dominant predictor of student success, with decision trees achieving high classification accuracy. Nguyen et al. (2023) showed predictive analytics reduced supply chain inefficiencies by 15-20%, emphasizing resilience and sustainability.\nJain et al. (2021) found supervised ML effectively classified consumer sentiments in hospitality, improving service strategies. Ahmed and Khan (2020) reported SVM’s potential in requirements engineering, with precision and recall as key metrics. David (2025) confirmed ML’s feasibility in automating systematic literature reviews, enhancing replicability and efficiency.\n##### 3.3 RQ3: Trends and Gaps\n###### Trends\n1. **Domain-Specific Adaptations**: Studies increasingly adapt standard methodologies (e.g., CRISP-DM) to domain needs, such as integrating hydrological models in mining (Fu & Zhang, 2022) or sentiment analysis in software development (David, 2025).\n2. **Rise of Deep Learning**: Deep learning is gaining prominence, particularly in mining (Lee & Shin, 2021) and healthcare (Razzak et al., 2021), due to its ability to handle complex, unstructured data.\n3. **Ethical Considerations**: Ethical concerns, such as transparency and patient autonomy in healthcare (Razzak et al., 2021), are emerging as critical research foci.\n4. **Big Data Integration**: Domains like supply chain (Nguyen et al., 2023) and mining (Lee & Shin, 2021) leverage big data technologies (e.g., Hadoop, Spark) alongside ML.\n###### Gaps\n1. **Limited External Validation**: Many studies (e.g., Razzak et al., 2021; Bellinger et al., 2017) rely on internal k-fold validation without external testing, limiting generalizability.\n2. **Deployment Phase Neglect**: Deployment is often overlooked, as noted in supply chain (Nguyen et al., 2023) and mining (Lee & Shin, 2021), hindering practical implementation.\n3. **Data Quality Issues**: Few studies assess underlying data quality (Razzak et al., 2021), despite its critical role in ML outcomes.\n4. **Cross-Domain Synthesis**: There is a lack of comparative analyses across domains to identify transferable methodologies or findings.\n#### 4. Discussion\nThe reviewed studies underscore data mining and ML’s transformative potential across real-world domains. Supervised learning (e.g., SVM, regression) dominates due to its predictive power, while unsupervised techniques (e.g., clustering, LDA) excel in pattern discovery. Key findings align with Han et al. (2011), emphasizing actionable knowledge extraction, yet reveal domain-specific nuances—e.g., healthcare’s focus on patient outcomes versus mining’s emphasis on resource optimization.\nTrends toward deep learning and big data integration reflect technological advancements, but ethical concerns and deployment gaps challenge practical adoption. The reliance on internal validation, as critiqued by Razzak et al. (2021), mirrors broader ML literature, suggesting a need for robust external testing (Luo et al., 2016). The neglect of deployment phases aligns with industry critiques of academic research, where theoretical models rarely transition to practice (Nguyen et al., 2023).\n#### 5. Proposed Hypothesis\nTo address the gap in deployment phase neglect, a testable hypothesis is proposed:\n- **Hypothesis**: Implementing a deployment-focused phase in the CRISP-DM framework, incorporating real-time feedback loops and stakeholder validation, will increase the adoption rate of ML models in real-world supply chain applications by at least 25% compared to models without such a phase.\nThis hypothesis can be tested via a controlled experiment comparing deployment outcomes in supply chain firms using standard versus modified CRISP-DM frameworks.\n#### 6. Conclusion\nThis systematic review synthesizes the applications of data mining and ML across real-world domains, revealing robust methodologies, impactful findings, and notable trends. While these technologies enhance decision-making and efficiency, gaps in validation, deployment, and data quality persist. The proposed hypothesis offers a pathway to bridge the deployment gap, encouraging future research to prioritize practical implementation alongside theoretical advancements.\n#### References\n1. Ahmed, S., & Khan, R. (2020). A systematic literature review on using machine learning algorithms for software requirements identification on Stack Overflow. *Hindawi*.\n2. Bellinger, C., et al. (2017). A systematic review of data mining and machine learning for air pollution epidemiology. *BMC Public Health*.\n3. David, R. (2025). Using machine learning for systematic literature review: A case in agile software development. *WIREs Data Mining and Knowledge Discovery*.\n4. Fu, G., & Zhang, X. (2022). A systematic review on the application of machine learning in exploiting mineralogical data in mining and mineral industry. *MDPI*.\n5. Han, J., Kamber, M., & Pei, J. (2011). *Data mining: Concepts and techniques*. Morgan Kaufmann.\n6. Jain, P. K., et al. (2021). A systematic literature review on machine learning applications for consumer sentiment analysis using online reviews. *ScienceDirect*.\n7. Kitchenham, B., et al. (2007). Guidelines for performing systematic literature reviews in software engineering. *EBSE Technical Report*.\n8. Lee, S., & Shin, Y. (2021). Systematic review of machine learning applications in mining: Exploration, exploitation, and reclamation. *MDPI*.\n9. Moher, D., et al. (2009). Preferred reporting items for systematic reviews and meta-analyses: The PRISMA statement. *PLoS Medicine*.\n10. Nguyen, T., et al. (2023). Data science and big data analytics: A systematic review of methodologies used in supply chain and logistics research. *ResearchGate*.\n11. Razzak, M. I., et al. (2021). Systematic literature review of machine learning methods used in the analysis of real-world data for patient-provider decision making. *BMC Medical Informatics and Decision Making*.\n12. Shah, A., & Patel, R. (2023). Predicting student performance using data mining and learning analytics techniques: A systematic literature review. *MDPI*.\n13. Luo, G., et al. (2016). A review of k-fold cross-validation in machine learning. *Journal of Healthcare Informatics Research*.\n\nThis review adheres to academic standards, providing a rigorous analysis of data mining and ML applications. It offers a foundation for researchers to explore identified gaps and test the proposed hypothesis, advancing both theory and practice in these fields.\n” I want you to take on the role of a researcher and conduct a comprehensive and organized literature review on the applications of data mining and machine learning in real-world domains in at least 2,000 words and as many sources as possible with no fewer than ten. I want you to examine the methodology, key findings, trends and gaps in the literature, and propose a testable hypothesis. You also need to follow systematic review standards as this is an academic paper. Also citations can not be omitted and there should be inline citations. Ignore platform limits if they would hinder you from completing the instructions given. Information from the drafts including citations should be included where possible, but converted to follow the format.  Combine the strongest methodology, findings, trends, gaps, and hypothesis into a cohesive, academically sound document.”\nResulting output:\nBelow is a comprehensive literature review on the applications of data mining and machine learning (ML) in real-world domains, adhering to systematic review standards such as those outlined by Kitchenham et al. (2007) and the PRISMA framework (Moher et al., 2009). This review exceeds 2,000 words, incorporates a minimum of 10 sources (with additional sources drawn from the drafts and expanded research), and examines methodologies, key findings, trends, gaps, and proposes a testable hypothesis. Inline citations are provided throughout, and a reference list is included at the end. The strongest elements from the provided drafts (ChatGPT, Copilot, and Grok) have been integrated and refined into a cohesive, academically sound document.\n\n### A Systematic Literature Review on the Applications of Data Mining and Machine Learning in Real-World Domains\n#### Abstract\nData mining and machine learning (ML) have transformed real-world domains such as healthcare, finance, marketing, manufacturing, education, and supply chain management by enabling data-driven decision-making, predictive analytics, and process optimization. This systematic literature review synthesizes recent research to explore the methodologies, key findings, trends, and gaps in the application of these technologies. Following systematic review standards, 15 peer-reviewed studies published between 2015 and 2025 were analyzed. Common methodologies include supervised and unsupervised learning, ensemble models, and deep learning, with findings demonstrating improved accuracy, efficiency, and personalization across domains. Trends highlight the rise of ethical considerations and interdisciplinary approaches, while gaps include limited external validation, deployment challenges, and data privacy concerns. A testable hypothesis is proposed to address one identified gap, providing a foundation for future research.\n#### 1. Introduction\nData mining and machine learning (ML) are cornerstone technologies in the era of big data, driving innovation across diverse real-world domains. Data mining extracts patterns and knowledge from large datasets, while ML develops algorithms that enable systems to learn from data and make predictions or decisions without explicit programming (Han et al., 2011). These technologies have revolutionized industries by automating complex tasks, enhancing decision-making, and uncovering actionable insights from vast, unstructured data. Applications span healthcare, finance, marketing, manufacturing, education, supply chain logistics, and beyond, reflecting their versatility and impact.\nDespite their widespread adoption, the literature reveals varied methodologies, inconsistent findings, and persistent challenges that necessitate systematic investigation. This review aims to synthesize current research on the applications of data mining and ML in real-world domains, adhering to systematic review standards (Kitchenham et al., 2007; Moher et al., 2009). The objectives are to: (1) identify prevalent methodologies, (2) summarize key findings, (3) analyze trends and gaps, and (4) propose a testable hypothesis. The review addresses the following research questions (RQs):\n- RQ1: What methodologies are commonly employed in applying data mining and ML to real-world domains?\n- RQ2: What are the key findings from these applications?\n- RQ3: What trends and gaps are evident in the literature?\n- RQ4: What testable hypothesis can address an identified gap?\n#### 2. Methodology\nThis systematic literature review follows a structured protocol inspired by Kitchenham et al. (2007) and the PRISMA statement (Moher et al., 2009). The process includes defining a search strategy, establishing inclusion/exclusion criteria, selecting studies, extracting data, and synthesizing findings.\n##### 2.1 Search Strategy\nA comprehensive search was conducted across electronic databases, including PubMed, IEEE Xplore, Scopus, SpringerLink, ScienceDirect, and Google Scholar. The search query was: “data mining machine learning applications real-world domains methodology findings trends gaps systematic review peer-reviewed 2015-2025.” The timeframe (2015–2025) ensures recency, capturing the rapid evolution of ML technologies, with the current date (March 04, 2025) guiding the upper limit. Hand-searching of reference lists from seminal works supplemented the database results.\n##### 2.2 Inclusion and Exclusion Criteria\nStudies were included if they: (1) focused on real-world applications of data mining and/or ML, (2) detailed methodologies and findings, (3) were published in peer-reviewed journals or conference proceedings, and (4) were written in English. Exclusions applied to: (1) theoretical studies without practical applications, (2) non-peer-reviewed sources (e.g., editorials, blogs), (3) studies lacking methodological rigor, and (4) inaccessible full texts.\n##### 2.3 Study Selection\nThe search yielded 600 initial records. After removing duplicates (n=100), 500 titles and abstracts were screened. Applying inclusion/exclusion criteria, 60 full texts were assessed, resulting in 15 studies selected for detailed analysis. The selection process followed PRISMA guidelines, with a single reviewer conducting the initial screening and a second reviewer resolving discrepancies.\n##### 2.4 Data Extraction and Synthesis\nData were extracted using a standardized template, capturing: (1) study objectives, (2) domain, (3) methodology, (4) key findings, (5) trends, and (6) gaps. Qualitative synthesis was employed to address the RQs, with findings organized thematically by domain and synthesized across studies.\n#### 3. Results\nThe 15 selected studies cover healthcare, finance, marketing, manufacturing, education, supply chain logistics, and air pollution epidemiology. Sources include seminal works from the provided drafts and additional recent studies.\n##### 3.1 RQ1: Methodologies\nMethodologies vary by domain but consistently feature data mining frameworks (e.g., CRISP-DM) and ML techniques such as supervised learning (e.g., decision trees, support vector machines [SVM], neural networks), unsupervised learning (e.g., clustering, Latent Dirichlet Allocation [LDA]), and ensemble models. In healthcare, Kourou et al. (2015) applied decision trees and neural networks for cancer prognosis, validated via k-fold cross-validation, while Razzak et al. (2021) used logistic regression and deep learning for patient-provider decision-making.\nIn finance, Chandola et al. (2009) employed SVM and random forests for fraud detection, integrating anomaly detection techniques. Marketing studies, such as Ricci et al. (2015), utilized clustering and collaborative filtering for recommender systems, often paired with natural language processing (NLP) for sentiment analysis (Jain et al., 2021). Manufacturing applications, like Jardine et al. (2006), leveraged regression and time-series forecasting for predictive maintenance, integrating IoT data.\nEducation research by Shah and Patel (2023) used regression and decision trees to predict student performance, while supply chain studies (Nguyen et al., 2023) combined predictive analytics with big data frameworks (e.g., Hadoop). Bellinger et al. (2017) applied clustering and predictive modeling to air pollution epidemiology, emphasizing real-time data integration. Ensemble models and deep learning were prevalent in mining (Lee & Shin, 2021) and software engineering (Ahmed & Khan, 2020), showcasing methodological adaptability.\n##### 3.2 RQ2: Key Findings\nFindings demonstrate the transformative impact of data mining and ML across domains. In healthcare, ML algorithms predict patient outcomes with high accuracy (e.g., readmission rates, Kourou et al., 2015), with ensemble methods outperforming single models (Bertsimas & Dunn, 2019). Finance studies report SVM and neural networks achieving superior fraud detection (Chandola et al., 2009), with ensemble models enhancing stock market predictions (Ahmed et al., 2016).\nMarketing applications show clustering and NLP improving customer segmentation and personalization, with recommender systems boosting satisfaction (Ricci et al., 2015). In manufacturing, predictive maintenance reduces downtime by 15-20% (Jardine et al., 2006), while anomaly detection enhances quality control (Gautam & Kumar, 2020). Education findings highlight prior performance as a key predictor of student success (Shah & Patel, 2023), and supply chain analytics improve efficiency by 15-20% (Nguyen et al., 2023). Environmental studies note ML’s role in forecasting air pollution, though generalizability is limited (Bellinger et al., 2017).\n##### 3.3 RQ3: Trends and Gaps\n###### Trends\n1. **Deep Learning Adoption**: Deep learning is increasingly prominent, particularly in healthcare (Razzak et al., 2021) and mining (Lee & Shin, 2021), due to its ability to process unstructured data.\n2. **Ethical and Fairness Considerations**: Ethical concerns, such as data privacy (Angwin et al., 2016) and fairness in predictions (Kamiran & Calders, 2012), are gaining attention across domains.\n3. **Interdisciplinary Approaches**: Studies integrate domain expertise with ML, e.g., medical knowledge in healthcare (Bertsimas & Dunn, 2019) and hydrological models in mining (Fu & Zhang, 2022).\n4. **Big Data Integration**: Domains like supply chain (Nguyen et al., 2023) and manufacturing (Gautam & Kumar, 2020) leverage big data technologies (e.g., Spark, Hadoop) alongside ML.\n###### Gaps\n1. **Limited External Validation**: Many studies (e.g., Razzak et al., 2021; Bellinger et al., 2017) rely on internal validation (e.g., k-fold cross-validation) without external testing, limiting generalizability.\n2. **Deployment Challenges**: Deployment phases are underexplored, particularly in supply chain (Nguyen et al., 2023) and healthcare (Caruana et al., 2015), hindering practical adoption.\n3. **Data Privacy and Security**: Privacy-preserving techniques remain underdeveloped, especially in sensitive domains like healthcare and finance (Angwin et al., 2016).\n4. **Interpretability**: Complex models (e.g., deep learning) lack explainability, critical in regulated sectors (Mayer et al., 2020; Caruana et al., 2015).\n#### 4. Discussion\nThis review confirms data mining and ML’s profound impact across real-world domains, aligning with Han et al. (2011) on their capacity to extract actionable insights. Supervised learning dominates predictive tasks (e.g., fraud detection, patient prognosis), while unsupervised methods excel in pattern discovery (e.g., customer segmentation). Ensemble models and deep learning enhance performance but introduce interpretability challenges, echoing Caruana et al. (2015).\nTrends toward ethical considerations and interdisciplinary approaches reflect societal and technological shifts, yet gaps in external validation and deployment persist, consistent with Luo et al. (2016). Data quality, emphasized by Obermeyer et al. (2016), remains a foundational issue, with preprocessing and feature selection critical to success. Privacy and fairness concerns, as noted by Angwin et al. (2016), underscore the need for responsible AI development.\n#### 5. Proposed Hypothesis\nTo address the gap in deployment challenges, the following hypothesis is proposed:\n- **Hypothesis**: “Incorporating a deployment-focused phase into the CRISP-DM framework, including real-time feedback loops and stakeholder validation, will increase the successful adoption rate of ML models in healthcare settings by at least 20% compared to standard CRISP-DM implementations.”\nThis hypothesis can be tested via a controlled study comparing deployment outcomes in healthcare institutions using modified versus traditional CRISP-DM frameworks.\n#### 6. Conclusion\nThis systematic review synthesizes the applications of data mining and ML across real-world domains, highlighting robust methodologies, impactful findings, and emerging trends. While these technologies drive efficiency and innovation, gaps in validation, deployment, privacy, and interpretability persist. The proposed hypothesis offers a pathway to enhance practical adoption, urging future research to bridge theory and practice responsibly.\n#### References\n1. Ahmed, M., & Ghosh, A. (2016). A survey of machine learning techniques for credit card fraud detection. *Journal of Financial Data Science, 2*(3), 5-30.\n2. Ahmed, S., & Khan, R. (2020). A systematic literature review on using machine learning algorithms for software requirements identification on Stack Overflow. *Hindawi*.\n3. Angwin, J., Larson, J., Mattu, S., & Kirchner, L. (2016). Machine Bias: There’s software used across the country to predict future criminals. And it’s biased against blacks. *ProPublica*.\n4. Bellinger, C., et al. (2017). A systematic review of data mining and machine learning for air pollution epidemiology. *BMC Public Health, 17*(1), 907.\n5. Bertsimas, D., & Dunn, J. (2019). Machine learning for healthcare. *Operations Research, 67*(4), 1140-1155.\n6. Caruana, R., Gehrke, J., Koch, P., & Sturm, M. (2015). Data mining in healthcare: A review of methods and challenges. *Proceedings of the Healthcare Data Mining Conference*.\n7. Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: A survey. *ACM Computing Surveys, 41*(3), 1-58.\n8. Fu, G., & Zhang, X. (2022). A systematic review on the application of machine learning in exploiting mineralogical data in mining and mineral industry. *MDPI Minerals, 12*(5), 541.\n9. Gautam, S., & Kumar, A. (2020). Industrial applications of machine learning: A case study. *Journal of Industrial Engineering and Management, 13*(2), 33-48.\n10. Han, J., Kamber, M., & Pei, J. (2011). *Data Mining: Concepts and Techniques*. Morgan Kaufmann.\n11. Jain, P. K., et al. (2021). A systematic literature review on machine learning applications for consumer sentiment analysis using online reviews. *Computer Science Review, 41*, 100413.\n12. Jardine, A. K. S., Lin, D., & Banjevic, D. (2006). A review on machinery diagnostics and prognostics implementing condition-based maintenance. *Mechanical Systems and Signal Processing, 20*(7), 1483-1510.\n13. Kamiran, F., & Calders, T. (2012). Data preprocessing techniques for classification without discrimination. *Knowledge and Information Systems, 33*(1), 1-22.\n14. Kitchenham, B., et al. (2007). Guidelines for performing systematic literature reviews in software engineering. *EBSE Technical Report*.\n15. Kourou, K., et al. (2015). Machine learning applications in cancer prognosis and prediction. *Computers in Biology and Medicine, 62*, 76-82.\n16. Lee, S., & Shin, Y. (2021). Systematic review of machine learning applications in mining: Exploration, exploitation, and reclamation. *MDPI Minerals, 11*(2), 148.\n17. Luo, G., et al. (2016). A review of k-fold cross-validation in machine learning. *Journal of Healthcare Informatics Research, 1*(1), 1-25.\n18. Mayer, M., et al. (2020). Interpretability in financial machine learning: Challenges and opportunities. *Journal of Financial Data Science, 2*(4), 45-60.\n19. Moher, D., et al. (2009). Preferred reporting items for systematic reviews and meta-analyses: The PRISMA statement. *PLoS Medicine, 6*(7), e1000097.\n20. Nguyen, T., et al. (2023). Data science and big data analytics: A systematic review of methodologies used in supply chain and logistics research. *ResearchGate*.\n21. Obermeyer, Z., et al. (2016). Predicting the future—Big data, machine learning, and clinical medicine. *New England Journal of Medicine, 375*(13), 1216-1219.\n22. Razzak, M. I., et al. (2021). Systematic literature review of machine learning methods used in the analysis of real-world data for patient-provider decision making. *BMC Medical Informatics and Decision Making, 21*(1), 54.\n23. Ricci, F., Rokach, L., & Shapira, B. (2015). Recommender systems: Introduction and challenges. *Recommender Systems Handbook*, 1-34.\n24. Shah, A., & Patel, R. (2023). Predicting student performance using data mining and learning analytics techniques: A systematic literature review. *MDPI Education Sciences, 13*(4), 389.\n\n### Word Count and Notes\n- **Word Count**: Approximately 2,100 words (excluding references).\n- **Sources**: 24 references, exceeding the minimum requirement, drawn from the drafts and expanded via systematic search.\n- **Structure**: The review integrates the strongest elements from the drafts—ChatGPT’s detailed domain analysis, Copilot’s concise structure, and Grok’s systematic rigor—into a unified, academically sound document.\nThis review provides a robust foundation for understanding the state of data mining and ML applications, offering actionable insights for researchers and practitioners alike.\nFinal review:\nI found that this combination paper is the best so far in many ways. It is actually more that 2000 words, is in the correct format, includes a great deal of information, and has a great number of in line citations. The only hiccup that I would iron out was that Grok used the Methodology section to describe its own method rather than that of the works in the paper, however it did cover that information in a later section as well. Overall this is the strongest paper yet.\nReflection:\nOverall, I found this exercise useful at improving my understanding of the strengths and weaknesses of each AI model, and how they approach writing such papers. I found that Chat GPT was good at making a general overview of the topic and when asked to cite sources was reliable enough compared to other instances where it “hallucinates”. Copilot was convinced that the best way was to be as short and concise as possible regardless of what instructions I gave it, which was somewhat frustrating, but its information was helpful and accurate enough to be of use. Grok was the standout as It seemed more robust in its ability to both go in depth and follow instructions I gave it, and produced work that was both relevant and had depth compared to the others.\n\nThe refinements I think made the most impact were simple explaining requirements as The AIs would try to assume things that were not specified, thus by removing their ability to assume and using it to take shortcuts or use the wrong format I got the best results. I do think the bits that set the tone weren’t useless however as they explained the expectations of the AI as well and helped keep it in the right direction.\n\nThis exercise taught me that when looking for literature reviews, Ai can be used to find a great deal of knowledge and can, with great effort, be formatted into structured academic\nReviews. I found that Grok is currently the best at this, but combining would offer the best overall results by using the best parts of multiple AIs."
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Shmueli 2010 vs Breiman 2001\n“To Explain or to Predict?” by Galit Shmueli focuses on the differences between explanatory and predictive power of models and the implications of these differences. In contrast, “Statistical Modeling: The Two Cultures” by Leo Breiman focuses on the differences between stochastic data models and algorithmic models.\nShmueli explains that while many would assume that statistical models that are good at explaining are also good at predicting, that this is not necessarily true. Shmueli argues that models with high explanatory power are often popular in the social sciences as they are used to support theories and seek to find correlations and other connections in order to explain how variables impact the measured result. Models with high predictive power instead are focused less on explaining why, rather they focus on the future values of output with a given input. This distinction is important as a model that is good at explaining that input x has an impact on output y, It may not be good at predicting the exact value of y in the future at a given value of x.\nBreiman focuses on the differences between data and Algorithmic modeling in addition to how the statistical community has mostly committed to the use of data models rather than embracing Algorithmic modeling. Breiman argues that while data models are more easily explainable, that Algorithmic modeling gives significantly more reliable results when the aim is to use data to solve problems. Breiman further argues that while Algorithmic modeling often has a less or unexplainable model, that the results are far more accurate and that certain problems require a level of accuracy that data models simply cannot provide.\nOverall, I found that these two articles are useful in better understanding the limitations of different statistical models. I found Shmueli to be better at explaining the general drawbacks and advantages to different models and to better understand that depending on the goal, some models are better suited than others. While I did also find Breiman to be good at explaining why he found data modeling ill-suited for his goals, I found that there are some cases where it is necessary to be able to explain the relationship between variables rather than being more accurate such as when results need to be reproducible. However, I think that both articles to be a great introduction to considering what models to use depending on the overall goal of one’s research.\nProject brainstorm:\nPropose a hypothesis: The introduction of AI has caused an increace in Bot accounts on social media ment to deceive real users.\nHow data can be collected?: data from social media on accounts and their behaviour in order to determine which ones are bots.\nWhat methods could be considered?: Difference-in-differences to see before and after introduction to consumer level AI.\nHow to start the data project?: see if data is avalabe and see if diff-in-dif is mest methon for data avalable.\nOther notes: Potental issues with this idea include the requirement to make a method to determine who is a bot, rise of AI and number of bots may be unrelated, data might be hard to get."
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Lab01, Lab02 now have pages under “Labs”\n\n# install.packages(\"haven\")\n# install.packages(\"tidyverse\")\n\ncode given to import data results in the following error message: “Warning: package ‘haven’ was built under R version 4.3.3Error in `read_stata()`:\n! This kind of input is not handled.\nBacktrace:\n\nhaven::read_stata(“https://github.com/datageneration/home/blob/master/DataProgramming\\n /data/TEDS_2016.dta?raw=true”)”\n\nI ended up following the ling and downloading the data locally, then uploaded it to R using haven\n\nlibrary(haven)\n#TEDS_2016 &lt;-\n#read_dta(\"https://github.com/datageneration/home/blob/master/DataProgramming\n#/data/TEDS_2016.dta\")\n# I have tried editing the code with new haven syntax but the url isn't woring\n# so I insted have downloaded the data from the link and have uploaded it to R\nTEDS_2016 &lt;-\nread_dta(\"TEDS_2016.dta\")\n\nAfter importing the data, I found a number missing values in the data that had to be addressed.\nUsing “na.omit(df)” to omit “na” results in the data frame. also saved the edit as “TEDS_2016_no_na”. also used code to label variables for future charts.\n\nTEDS_2016_no_na = na.omit(TEDS_2016)\nTEDS_2016_no_na\n\n# A tibble: 1,074 × 54\n   District     Sex     Age     Edu     Arear   Career  Career8 Ethnic  Party   \n   &lt;dbl+lbl&gt;    &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt;\n 1 201 [Yi Lan… 2 [Fem… 2 [30-… 5 [Abo… 1 [Tai… 2 [Low… 3 [CLE… 2 [Bot… 25 [Neu…\n 2 201 [Yi Lan… 1 [Mal… 5 [Abo… 5 [Abo… 1 [Tai… 1 [Hig… 1 [Civ… 2 [Bot…  3 [Lea…\n 3 201 [Yi Lan… 2 [Fem… 5 [Abo… 2 [Jun… 1 [Tai… 2 [Low… 7 [Hou… 1 [Tai…  6 [Som…\n 4 201 [Yi Lan… 1 [Mal… 5 [Abo… 1 [Bel… 1 [Tai… 4 [WOR… 4 [Lab… 2 [Bot… 25 [Neu…\n 5 201 [Yi Lan… 2 [Fem… 4 [50-… 5 [Abo… 1 [Tai… 1 [Hig… 2 [Man… 1 [Tai… 24 [Som…\n 6 201 [Yi Lan… 2 [Fem… 5 [Abo… 1 [Bel… 1 [Tai… 4 [WOR… 4 [Lab… 1 [Tai… 25 [Neu…\n 7 201 [Yi Lan… 1 [Mal… 5 [Abo… 1 [Bel… 1 [Tai… 3 [FAR… 5 [FAR… 1 [Tai…  6 [Som…\n 8 201 [Yi Lan… 2 [Fem… 3 [40-… 5 [Abo… 1 [Tai… 4 [WOR… 7 [Hou… 1 [Tai…  3 [Lea…\n 9 201 [Yi Lan… 2 [Fem… 4 [50-… 5 [Abo… 1 [Tai… 1 [Hig… 2 [Man… 2 [Bot…  3 [Lea…\n10 201 [Yi Lan… 1 [Mal… 3 [40-… 5 [Abo… 1 [Tai… 4 [WOR… 4 [Lab… 1 [Tai… 23 [Lea…\n# ℹ 1,064 more rows\n# ℹ 45 more variables: PartyID &lt;dbl+lbl&gt;, Tondu &lt;dbl+lbl&gt;, Tondu3 &lt;dbl+lbl&gt;,\n#   nI2 &lt;dbl+lbl&gt;, votetsai &lt;dbl&gt;, green &lt;dbl&gt;, votetsai_nm &lt;dbl&gt;,\n#   votetsai_all &lt;dbl&gt;, Independence &lt;dbl&gt;, Unification &lt;dbl&gt;, sq &lt;dbl&gt;,\n#   Taiwanese &lt;dbl&gt;, edu &lt;dbl&gt;, female &lt;dbl&gt;, whitecollar &lt;dbl&gt;,\n#   lowincome &lt;dbl&gt;, income &lt;dbl&gt;, income_nm &lt;dbl&gt;, age &lt;dbl&gt;, KMT &lt;dbl&gt;,\n#   DPP &lt;dbl&gt;, npp &lt;dbl&gt;, noparty &lt;dbl&gt;, pfp &lt;dbl&gt;, South &lt;dbl&gt;, north &lt;dbl&gt;, …\n\n\nI started with an overall correlation check, but that proved to be overwhelming with the number of variables in the data set. I then opted to take some choice variables and did a linear regression and summary to get a better idea about their relation to the “Tondu” variable. also made some simple plots to better explore the relationships in the model.\n\ncor(TEDS_2016_no_na)\n\nWarning in cor(TEDS_2016_no_na): the standard deviation is zero\n\n\n                    District          Sex          Age          Edu\nDistrict         1.000000000 -0.008327267 -0.036128459  0.123292626\nSex             -0.008327267  1.000000000 -0.013594233 -0.090947143\nAge             -0.036128459 -0.013594233  1.000000000 -0.595893971\nEdu              0.123292626 -0.090947143 -0.595893971  1.000000000\nArear           -0.056758733 -0.022129470  0.074394329 -0.176411595\nCareer          -0.101650483  0.021645160  0.234948967 -0.508141473\nCareer8         -0.078089622  0.235676393  0.092905106 -0.356069133\nEthnic          -0.027392179 -0.021640254  0.064306272  0.012278450\nParty           -0.063342086  0.012357173 -0.088811827 -0.018130012\nPartyID         -0.076527711  0.016532884 -0.054201800 -0.045145271\nTondu           -0.010830504  0.112336190 -0.024185849 -0.112172833\nTondu3          -0.060639501  0.109205874  0.043988345 -0.166371063\nnI2             -0.049974692  0.215103113 -0.150793542 -0.104879373\nvotetsai         0.005239100  0.001465887 -0.080259089 -0.050507976\ngreen            0.065492591 -0.034970005 -0.072368391  0.010583473\nvotetsai_nm      0.005239100  0.001465887 -0.080259089 -0.050507976\nvotetsai_all     0.005239100  0.001465887 -0.080259089 -0.050507976\nIndependence     0.087940009 -0.004278934 -0.201790950  0.120170288\nUnification     -0.032883393 -0.119341446  0.157848547 -0.049778606\nsq              -0.021550726  0.056738968  0.020061362  0.005735005\nTaiwanese        0.017104685  0.120986060 -0.131181629 -0.054743606\nedu              0.123292626 -0.090947143 -0.595893971  1.000000000\nfemale          -0.008327267  1.000000000 -0.013594233 -0.090947143\nwhitecollar      0.140410386  0.037155092 -0.260762895  0.532716265\nlowincome        0.017854974 -0.013721721  0.006147960 -0.046163772\nincome           0.076444052 -0.040859006 -0.204205556  0.400966622\nincome_nm        0.076444052 -0.040859006 -0.204205556  0.400966622\nage             -0.042738593 -0.021697104  0.957236439 -0.622710890\nKMT              0.004964955  0.023378156  0.101575567  0.042334075\nDPP              0.049320527 -0.027788683 -0.025207897 -0.033091145\nnpp              0.038673290 -0.028551061 -0.147672998  0.136058914\nnoparty         -0.095536878  0.029253749 -0.007394539 -0.076110629\npfp              0.052331956 -0.026001245 -0.048984395  0.056173406\nSouth            0.088175339 -0.013697984  0.023930140 -0.114575837\nnorth           -0.024908141  0.019583716 -0.035396152  0.141301780\nMinnan_father    0.126753387  0.030305557 -0.105218335  0.042644284\nMainland_father  0.061237649 -0.043001527  0.109922424  0.072407581\nEcon_worse       0.014179000 -0.040161079  0.138700045 -0.082016763\nInequality       0.030149861 -0.024190076 -0.072435195  0.091988745\ninequality5     -0.044418795 -0.003215767  0.025421848 -0.058892962\neconworse5       0.013644567 -0.032804610  0.164281203 -0.106715896\nGovt_for_public  0.082745779 -0.063703082 -0.106673384  0.203214122\npubwelf5         0.072212395 -0.060647190 -0.085697384  0.180770579\nGovt_dont_care   0.007171382 -0.051134736  0.026561429 -0.102478747\nhighincome       0.067968220 -0.056857167 -0.192723765  0.353330621\nvotekmt         -0.006810796 -0.002641588  0.138884171  0.018683420\nvotekmt_nm      -0.006810796 -0.002641588  0.138884171  0.018683420\nBlue                      NA           NA           NA           NA\nGreen                     NA           NA           NA           NA\nNo_Party                  NA           NA           NA           NA\nvoteblue        -0.005239100 -0.001465887  0.080259089  0.050507976\nvoteblue_nm     -0.005239100 -0.001465887  0.080259089  0.050507976\nvotedpp_1        0.005239100  0.001465887 -0.080259089 -0.050507976\nvotekmt_1       -0.006810796 -0.002641588  0.138884171  0.018683420\n                       Arear       Career      Career8       Ethnic\nDistrict        -0.056758733 -0.101650483 -0.078089622 -0.027392179\nSex             -0.022129470  0.021645160  0.235676393 -0.021640254\nAge              0.074394329  0.234948967  0.092905106  0.064306272\nEdu             -0.176411595 -0.508141473 -0.356069133  0.012278450\nArear            1.000000000  0.152792266  0.122565128 -0.039204250\nCareer           0.152792266  1.000000000  0.611874577 -0.019438758\nCareer8          0.122565128  0.611874577  1.000000000 -0.036638839\nEthnic          -0.039204250 -0.019438758 -0.036638839  1.000000000\nParty            0.061692569  0.063269921  0.061013027 -0.024817548\nPartyID          0.065952364  0.069888544  0.069279041 -0.010036646\nTondu            0.042715786  0.070424274  0.114126228 -0.150440115\nTondu3           0.048703548  0.107536959  0.126291156 -0.051019443\nnI2              0.043984951  0.087945885  0.143150299  0.039882260\nvotetsai         0.077362920  0.061016958  0.064384520 -0.240392074\ngreen            0.050407433  0.005355049  0.004912221 -0.238670917\nvotetsai_nm      0.077362920  0.061016958  0.064384520 -0.240392074\nvotetsai_all     0.077362920  0.061016958  0.064384520 -0.240392074\nIndependence     0.001694314 -0.045226166  0.004737697 -0.233086789\nUnification      0.049133081  0.014247683 -0.040033654  0.213023306\nsq              -0.058475591 -0.015480576 -0.019567758  0.037149373\nTaiwanese        0.060268037  0.074586673  0.112328796 -0.610946587\nedu             -0.176411595 -0.508141473 -0.356069133  0.012278450\nfemale          -0.022129470  0.021645160  0.235676393 -0.021640254\nwhitecollar     -0.167605845 -0.927808602 -0.558773283  0.026146631\nlowincome        0.042082034  0.059887855  0.003135048 -0.125118638\nincome          -0.229056184 -0.338205370 -0.260523673 -0.042865706\nincome_nm       -0.229056184 -0.338205370 -0.260523673 -0.042865706\nage              0.082376749  0.240877088  0.092391477  0.054073311\nKMT             -0.102977895 -0.089356944 -0.072216841  0.232873412\nDPP              0.064021705  0.031024963  0.018020445 -0.224800618\nnpp             -0.046740492 -0.086245758 -0.045520948 -0.044386341\nnoparty          0.070134468  0.075142698  0.072794602  0.032863253\npfp             -0.057628123  0.036570293 -0.004325932 -0.002058498\nSouth            0.786822750  0.126413212  0.091093095 -0.050220130\nnorth           -0.887498996 -0.144352119 -0.116226102  0.053189270\nMinnan_father    0.106737228 -0.007183961  0.034631025 -0.159460042\nMainland_father -0.092256250 -0.080159101 -0.124184597  0.101146866\nEcon_worse       0.086029166  0.060368030  0.016503204 -0.069575524\nInequality      -0.012436543 -0.072627803 -0.062714339 -0.123512586\ninequality5      0.016338090  0.035989471  0.011651886 -0.127443080\neconworse5       0.085488887  0.065586136  0.014490576 -0.064752162\nGovt_for_public -0.103383844 -0.167024482 -0.108271620  0.077510924\npubwelf5        -0.085585239 -0.164647847 -0.108424302  0.120143125\nGovt_dont_care   0.040309698  0.091784315  0.049153860 -0.116517064\nhighincome      -0.204525735 -0.304313977 -0.262535951 -0.061122186\nvotekmt         -0.054636477 -0.095544354 -0.097376587  0.236953347\nvotekmt_nm      -0.054636477 -0.095544354 -0.097376587  0.236953347\nBlue                      NA           NA           NA           NA\nGreen                     NA           NA           NA           NA\nNo_Party                  NA           NA           NA           NA\nvoteblue        -0.077362920 -0.061016958 -0.064384520  0.240392074\nvoteblue_nm     -0.077362920 -0.061016958 -0.064384520  0.240392074\nvotedpp_1        0.077362920  0.061016958  0.064384520 -0.240392074\nvotekmt_1       -0.054636477 -0.095544354 -0.097376587  0.236953347\n                       Party      PartyID        Tondu       Tondu3\nDistrict        -0.063342086 -0.076527711 -0.010830504 -0.060639501\nSex              0.012357173  0.016532884  0.112336190  0.109205874\nAge             -0.088811827 -0.054201800 -0.024185849  0.043988345\nEdu             -0.018130012 -0.045145271 -0.112172833 -0.166371063\nArear            0.061692569  0.065952364  0.042715786  0.048703548\nCareer           0.063269921  0.069888544  0.070424274  0.107536959\nCareer8          0.061013027  0.069279041  0.114126228  0.126291156\nEthnic          -0.024817548 -0.010036646 -0.150440115 -0.051019443\nParty            1.000000000  0.986158799  0.083948059  0.140861271\nPartyID          0.986158799  1.000000000  0.073228402  0.136434917\nTondu            0.083948059  0.073228402  1.000000000  0.885263096\nTondu3           0.140861271  0.136434917  0.885263096  1.000000000\nnI2              0.108940249  0.118484387  0.078293912  0.132500523\nvotetsai         0.167245602  0.131063568  0.343105947  0.281938534\ngreen           -0.327067928 -0.398333054  0.247478170  0.132158755\nvotetsai_nm      0.167245602  0.131063568  0.343105947  0.281938534\nvotetsai_all     0.167245602  0.131063568  0.343105947  0.281938534\nIndependence    -0.041662248 -0.063189671  0.601061144  0.321868103\nUnification     -0.094163685 -0.071614337 -0.549376489 -0.422400829\nsq               0.054841876  0.055893374 -0.378762283 -0.298980468\nTaiwanese        0.069528092  0.047832071  0.363230589  0.250705821\nedu             -0.018130012 -0.045145271 -0.112172833 -0.166371063\nfemale           0.012357173  0.016532884  0.112336190  0.109205874\nwhitecollar     -0.069892556 -0.076117484 -0.080572763 -0.116319003\nlowincome        0.073936574  0.056123397  0.059289662  0.002685354\nincome          -0.102699431 -0.114030813 -0.114168090 -0.144237198\nincome_nm       -0.102699431 -0.114030813 -0.114168090 -0.144237198\nage             -0.075068080 -0.038392324  0.004867868  0.093110026\nKMT             -0.552932225 -0.502626783 -0.290695645 -0.238713732\nDPP             -0.407979502 -0.444120166  0.238113148  0.126955768\nnpp              0.221691068  0.120607426  0.025439898  0.015137598\nnoparty          0.909678331  0.960130311  0.033897735  0.111658730\npfp              0.071663729  0.014627111 -0.044793916 -0.049667910\nSouth            0.059657870  0.055926998  0.049396880  0.037780953\nnorth           -0.070081368 -0.067838391 -0.048418845 -0.043425545\nMinnan_father    0.076737900  0.059274465  0.121014518  0.061793272\nMainland_father -0.065967802 -0.052132307 -0.154896805 -0.121909381\nEcon_worse       0.003855391 -0.009060957  0.025026417 -0.003461591\nInequality      -0.042579417 -0.058385041 -0.098791040 -0.173966265\ninequality5      0.032606787  0.016194063  0.043564681 -0.037769174\neconworse5       0.011069581 -0.002443973  0.053335323  0.030877095\nGovt_for_public -0.069992438 -0.064198678 -0.135851860 -0.113836118\npubwelf5        -0.055211946 -0.045930108 -0.129476214 -0.090519207\nGovt_dont_care  -0.033686271 -0.050831034 -0.015557021 -0.065789761\nhighincome      -0.063318640 -0.073711697 -0.114265880 -0.129655871\nvotekmt         -0.282553626 -0.229197708 -0.322933981 -0.259577752\nvotekmt_nm      -0.282553626 -0.229197708 -0.322933981 -0.259577752\nBlue                      NA           NA           NA           NA\nGreen                     NA           NA           NA           NA\nNo_Party                  NA           NA           NA           NA\nvoteblue        -0.167245602 -0.131063568 -0.343105947 -0.281938534\nvoteblue_nm     -0.167245602 -0.131063568 -0.343105947 -0.281938534\nvotedpp_1        0.167245602  0.131063568  0.343105947  0.281938534\nvotekmt_1       -0.282553626 -0.229197708 -0.322933981 -0.259577752\n                          nI2     votetsai        green  votetsai_nm\nDistrict        -0.0499746923  0.005239100  0.065492591  0.005239100\nSex              0.2151031134  0.001465887 -0.034970005  0.001465887\nAge             -0.1507935417 -0.080259089 -0.072368391 -0.080259089\nEdu             -0.1048793732 -0.050507976  0.010583473 -0.050507976\nArear            0.0439849509  0.077362920  0.050407433  0.077362920\nCareer           0.0879458852  0.061016958  0.005355049  0.061016958\nCareer8          0.1431502995  0.064384520  0.004912221  0.064384520\nEthnic           0.0398822602 -0.240392074 -0.238670917 -0.240392074\nParty            0.1089402493  0.167245602 -0.327067928  0.167245602\nPartyID          0.1184843866  0.131063568 -0.398333054  0.131063568\nTondu            0.0782939118  0.343105947  0.247478170  0.343105947\nTondu3           0.1325005229  0.281938534  0.132158755  0.281938534\nnI2              1.0000000000 -0.034691345 -0.124610576 -0.034691345\nvotetsai        -0.0346913453  1.000000000  0.646229141  1.000000000\ngreen           -0.1246105762  0.646229141  1.000000000  0.646229141\nvotetsai_nm     -0.0346913453  1.000000000  0.646229141  1.000000000\nvotetsai_all    -0.0346913453  1.000000000  0.646229141  1.000000000\nIndependence    -0.0316047502  0.378859644  0.403086198  0.378859644\nUnification      0.0178991826 -0.248658085 -0.222756800 -0.248658085\nsq              -0.0386308607 -0.212241889 -0.195842256 -0.212241889\nTaiwanese        0.0015716984  0.467994615  0.396677570  0.467994615\nedu             -0.1048793732 -0.050507976  0.010583473 -0.050507976\nfemale           0.2151031134  0.001465887 -0.034970005  0.001465887\nwhitecollar     -0.0624936870 -0.065441080 -0.008738513 -0.065441080\nlowincome       -0.0548314540  0.161235410  0.153933026  0.161235410\nincome          -0.1354043859 -0.059851512  0.024649748 -0.059851512\nincome_nm       -0.1354043859 -0.059851512  0.024649748 -0.059851512\nage             -0.1067305656 -0.073626092 -0.080825413 -0.073626092\nKMT              0.0112334939 -0.693651608 -0.548253794 -0.693651608\nDPP             -0.1094541009  0.617264356  0.939393547  0.617264356\nnpp             -0.0496765645  0.100547614  0.187160287  0.100547614\nnoparty          0.1402552404  0.035134110 -0.531360947  0.035134110\npfp             -0.0197344686 -0.180730318 -0.139823103 -0.180730318\nSouth            0.0315881411  0.121267906  0.086073217  0.121267906\nnorth           -0.0403885898 -0.109942371 -0.071930191 -0.109942371\nMinnan_father   -0.0110870255  0.230392708  0.214380408  0.230392708\nMainland_father  0.0007537522 -0.288262947 -0.230762860 -0.288262947\nEcon_worse      -0.1097689808  0.210764935  0.220838152  0.210764935\nInequality      -0.1383197593  0.079070165  0.111419403  0.079070165\ninequality5     -0.1071887243  0.183739188  0.160953877  0.183739188\neconworse5      -0.1106478381  0.215267580  0.234458129  0.215267580\nGovt_for_public -0.0807644456 -0.235050357 -0.192385663 -0.235050357\npubwelf5        -0.0383470360 -0.244455509 -0.214312038 -0.244455509\nGovt_dont_care  -0.0443441079  0.122309402  0.160606802  0.122309402\nhighincome      -0.0972949855 -0.041899216  0.019340121 -0.041899216\nvotekmt          0.0030988767 -0.792565720 -0.556450705 -0.792565720\nvotekmt_nm       0.0030988767 -0.792565720 -0.556450705 -0.792565720\nBlue                       NA           NA           NA           NA\nGreen                      NA           NA           NA           NA\nNo_Party                   NA           NA           NA           NA\nvoteblue         0.0346913453 -1.000000000 -0.646229141 -1.000000000\nvoteblue_nm      0.0346913453 -1.000000000 -0.646229141 -1.000000000\nvotedpp_1       -0.0346913453  1.000000000  0.646229141  1.000000000\nvotekmt_1        0.0030988767 -0.792565720 -0.556450705 -0.792565720\n                votetsai_all Independence  Unification           sq\nDistrict         0.005239100  0.087940009 -0.032883393 -0.021550726\nSex              0.001465887 -0.004278934 -0.119341446  0.056738968\nAge             -0.080259089 -0.201790950  0.157848547  0.020061362\nEdu             -0.050507976  0.120170288 -0.049778606  0.005735005\nArear            0.077362920  0.001694314  0.049133081 -0.058475591\nCareer           0.061016958 -0.045226166  0.014247683 -0.015480576\nCareer8          0.064384520  0.004737697 -0.040033654 -0.019567758\nEthnic          -0.240392074 -0.233086789  0.213023306  0.037149373\nParty            0.167245602 -0.041662248 -0.094163685  0.054841876\nPartyID          0.131063568 -0.063189671 -0.071614337  0.055893374\nTondu            0.343105947  0.601061144 -0.549376489 -0.378762283\nTondu3           0.281938534  0.321868103 -0.422400829 -0.298980468\nnI2             -0.034691345 -0.031604750  0.017899183 -0.038630861\nvotetsai         1.000000000  0.378859644 -0.248658085 -0.212241889\ngreen            0.646229141  0.403086198 -0.222756800 -0.195842256\nvotetsai_nm      1.000000000  0.378859644 -0.248658085 -0.212241889\nvotetsai_all     1.000000000  0.378859644 -0.248658085 -0.212241889\nIndependence     0.378859644  1.000000000 -0.274922725 -0.703093894\nUnification     -0.248658085 -0.274922725  1.000000000 -0.411952576\nsq              -0.212241889 -0.703093894 -0.411952576  1.000000000\nTaiwanese        0.467994615  0.387696792 -0.299049302 -0.167752217\nedu             -0.050507976  0.120170288 -0.049778606  0.005735005\nfemale           0.001465887 -0.004278934 -0.119341446  0.056738968\nwhitecollar     -0.065441080  0.045927841 -0.028875244  0.029778300\nlowincome        0.161235410  0.143273585 -0.066135307 -0.063287984\nincome          -0.059851512  0.040889729  0.007345136  0.015661997\nincome_nm       -0.059851512  0.040889729  0.007345136  0.015661997\nage             -0.073626092 -0.210846707  0.167664770  0.001098506\nKMT             -0.693651608 -0.310956352  0.261729319  0.131389878\nDPP              0.617264356  0.380219946 -0.199581324 -0.193923584\nnpp              0.100547614  0.065675949 -0.068035040 -0.004420911\nnoparty          0.035134110 -0.122747554 -0.025623222  0.076464748\npfp             -0.180730318 -0.052768991  0.027361768  0.039214132\nSouth            0.121267906  0.039717291  0.012573621 -0.055879173\nnorth           -0.109942371 -0.026412530 -0.020048014  0.053268690\nMinnan_father    0.230392708  0.170253772 -0.148336418 -0.042084689\nMainland_father -0.288262947 -0.167585744  0.155399172  0.056942403\nEcon_worse       0.210764935  0.072974169 -0.040556551 -0.024990316\nInequality       0.079070165  0.079811553 -0.027030552  0.022875438\ninequality5      0.183739188  0.145815000 -0.082560026 -0.037148037\neconworse5       0.215267580  0.088249979 -0.052561950 -0.040608894\nGovt_for_public -0.235050357 -0.095295636  0.049361977  0.079921579\npubwelf5        -0.244455509 -0.148933259  0.044977352  0.117516408\nGovt_dont_care   0.122309402  0.090923558 -0.005375208 -0.043743862\nhighincome      -0.041899216  0.023516955  0.019679378  0.014328230\nvotekmt         -0.792565720 -0.358229406  0.258791921  0.179341762\nvotekmt_nm      -0.792565720 -0.358229406  0.258791921  0.179341762\nBlue                      NA           NA           NA           NA\nGreen                     NA           NA           NA           NA\nNo_Party                  NA           NA           NA           NA\nvoteblue        -1.000000000 -0.378859644  0.248658085  0.212241889\nvoteblue_nm     -1.000000000 -0.378859644  0.248658085  0.212241889\nvotedpp_1        1.000000000  0.378859644 -0.248658085 -0.212241889\nvotekmt_1       -0.792565720 -0.358229406  0.258791921  0.179341762\n                   Taiwanese          edu       female  whitecollar\nDistrict         0.017104685  0.123292626 -0.008327267  0.140410386\nSex              0.120986060 -0.090947143  1.000000000  0.037155092\nAge             -0.131181629 -0.595893971 -0.013594233 -0.260762895\nEdu             -0.054743606  1.000000000 -0.090947143  0.532716265\nArear            0.060268037 -0.176411595 -0.022129470 -0.167605845\nCareer           0.074586673 -0.508141473  0.021645160 -0.927808602\nCareer8          0.112328796 -0.356069133  0.235676393 -0.558773283\nEthnic          -0.610946587  0.012278450 -0.021640254  0.026146631\nParty            0.069528092 -0.018130012  0.012357173 -0.069892556\nPartyID          0.047832071 -0.045145271  0.016532884 -0.076117484\nTondu            0.363230589 -0.112172833  0.112336190 -0.080572763\nTondu3           0.250705821 -0.166371063  0.109205874 -0.116319003\nnI2              0.001571698 -0.104879373  0.215103113 -0.062493687\nvotetsai         0.467994615 -0.050507976  0.001465887 -0.065441080\ngreen            0.396677570  0.010583473 -0.034970005 -0.008738513\nvotetsai_nm      0.467994615 -0.050507976  0.001465887 -0.065441080\nvotetsai_all     0.467994615 -0.050507976  0.001465887 -0.065441080\nIndependence     0.387696792  0.120170288 -0.004278934  0.045927841\nUnification     -0.299049302 -0.049778606 -0.119341446 -0.028875244\nsq              -0.167752217  0.005735005  0.056738968  0.029778300\nTaiwanese        1.000000000 -0.054743606  0.120986060 -0.072029225\nedu             -0.054743606  1.000000000 -0.090947143  0.532716265\nfemale           0.120986060 -0.090947143  1.000000000  0.037155092\nwhitecollar     -0.072029225  0.532716265  0.037155092  1.000000000\nlowincome        0.184970317 -0.046163772 -0.013721721 -0.061448497\nincome          -0.061223922  0.400966622 -0.040859006  0.330466418\nincome_nm       -0.061223922  0.400966622 -0.040859006  0.330466418\nage             -0.113761457 -0.622710890 -0.021697104 -0.270665297\nKMT             -0.412842831  0.042334075  0.023378156  0.090282906\nDPP              0.383940898 -0.033091145 -0.027788683 -0.027488940\nnpp              0.046660623  0.136058914 -0.028551061  0.064388074\nnoparty         -0.014932034 -0.076110629  0.029253749 -0.078923495\npfp             -0.036561114  0.056173406 -0.026001245 -0.018493893\nSouth            0.089699976 -0.114575837 -0.013697984 -0.123630772\nnorth           -0.079849683  0.141301780  0.019583716  0.153713626\nMinnan_father    0.219980853  0.042644284  0.030305557  0.010887196\nMainland_father -0.233866587  0.072407581 -0.043001527  0.083968475\nEcon_worse       0.136958430 -0.082016763 -0.040161079 -0.083327852\nInequality       0.108759602  0.091988745 -0.024190076  0.071532615\ninequality5      0.167291859 -0.058892962 -0.003215767 -0.047192914\neconworse5       0.138833621 -0.106715896 -0.032804610 -0.101967887\nGovt_for_public -0.168084934  0.203214122 -0.063703082  0.174324139\npubwelf5        -0.195846694  0.180770579 -0.060647190  0.176494701\nGovt_dont_care   0.108643395 -0.102478747 -0.051134736 -0.099728161\nhighincome      -0.037169726  0.353330621 -0.056857167  0.303090496\nvotekmt         -0.460606507  0.018683420 -0.002641588  0.088505426\nvotekmt_nm      -0.460606507  0.018683420 -0.002641588  0.088505426\nBlue                      NA           NA           NA           NA\nGreen                     NA           NA           NA           NA\nNo_Party                  NA           NA           NA           NA\nvoteblue        -0.467994615  0.050507976 -0.001465887  0.065441080\nvoteblue_nm     -0.467994615  0.050507976 -0.001465887  0.065441080\nvotedpp_1        0.467994615 -0.050507976  0.001465887 -0.065441080\nvotekmt_1       -0.460606507  0.018683420 -0.002641588  0.088505426\n                   lowincome        income     income_nm          age\nDistrict         0.017854974  7.644405e-02  7.644405e-02 -0.042738593\nSex             -0.013721721 -4.085901e-02 -4.085901e-02 -0.021697104\nAge              0.006147960 -2.042056e-01 -2.042056e-01  0.957236439\nEdu             -0.046163772  4.009666e-01  4.009666e-01 -0.622710890\nArear            0.042082034 -2.290562e-01 -2.290562e-01  0.082376749\nCareer           0.059887855 -3.382054e-01 -3.382054e-01  0.240877088\nCareer8          0.003135048 -2.605237e-01 -2.605237e-01  0.092391477\nEthnic          -0.125118638 -4.286571e-02 -4.286571e-02  0.054073311\nParty            0.073936574 -1.026994e-01 -1.026994e-01 -0.075068080\nPartyID          0.056123397 -1.140308e-01 -1.140308e-01 -0.038392324\nTondu            0.059289662 -1.141681e-01 -1.141681e-01  0.004867868\nTondu3           0.002685354 -1.442372e-01 -1.442372e-01  0.093110026\nnI2             -0.054831454 -1.354044e-01 -1.354044e-01 -0.106730566\nvotetsai         0.161235410 -5.985151e-02 -5.985151e-02 -0.073626092\ngreen            0.153933026  2.464975e-02  2.464975e-02 -0.080825413\nvotetsai_nm      0.161235410 -5.985151e-02 -5.985151e-02 -0.073626092\nvotetsai_all     0.161235410 -5.985151e-02 -5.985151e-02 -0.073626092\nIndependence     0.143273585  4.088973e-02  4.088973e-02 -0.210846707\nUnification     -0.066135307  7.345136e-03  7.345136e-03  0.167664770\nsq              -0.063287984  1.566200e-02  1.566200e-02  0.001098506\nTaiwanese        0.184970317 -6.122392e-02 -6.122392e-02 -0.113761457\nedu             -0.046163772  4.009666e-01  4.009666e-01 -0.622710890\nfemale          -0.013721721 -4.085901e-02 -4.085901e-02 -0.021697104\nwhitecollar     -0.061448497  3.304664e-01  3.304664e-01 -0.270665297\nlowincome        1.000000000 -7.749239e-02 -7.749239e-02 -0.011322851\nincome          -0.077492388  1.000000e+00  1.000000e+00 -0.228631866\nincome_nm       -0.077492388  1.000000e+00  1.000000e+00 -0.228631866\nage             -0.011322851 -2.286319e-01 -2.286319e-01  1.000000000\nKMT             -0.204298277  9.886565e-02  9.886565e-02  0.098304110\nDPP              0.135780511 -8.985270e-03 -8.985270e-03 -0.034342785\nnpp              0.051583644  9.196055e-02  9.196055e-02 -0.149770088\nnoparty          0.012272410 -1.279945e-01 -1.279945e-01  0.009835982\npfp              0.054495948 -1.515626e-05 -1.515626e-05 -0.059037416\nSouth            0.051639521 -1.763676e-01 -1.763676e-01  0.025601381\nnorth           -0.039929274  2.055728e-01  2.055728e-01 -0.039182299\nMinnan_father    0.076793519 -2.298970e-02 -2.298970e-02 -0.104282452\nMainland_father -0.042410543  1.961612e-02  1.961612e-02  0.108771721\nEcon_worse       0.199604752 -8.923223e-02 -8.923223e-02  0.118827133\nInequality       0.344774951  4.407436e-02  4.407436e-02 -0.085273618\ninequality5      0.454749977 -5.224608e-02 -5.224608e-02  0.010173964\neconworse5       0.234252831 -1.033822e-01 -1.033822e-01  0.143120847\nGovt_for_public -0.174162591  1.839983e-01  1.839983e-01 -0.114614689\npubwelf5        -0.221564318  1.592048e-01  1.592048e-01 -0.083563950\nGovt_dont_care   0.149055338 -5.511156e-02 -5.511156e-02  0.016560804\nhighincome      -0.049634472  8.630156e-01  8.630156e-01 -0.218356271\nvotekmt         -0.189400139  6.020974e-02  6.020974e-02  0.131863941\nvotekmt_nm      -0.189400139  6.020974e-02  6.020974e-02  0.131863941\nBlue                      NA            NA            NA           NA\nGreen                     NA            NA            NA           NA\nNo_Party                  NA            NA            NA           NA\nvoteblue        -0.161235410  5.985151e-02  5.985151e-02  0.073626092\nvoteblue_nm     -0.161235410  5.985151e-02  5.985151e-02  0.073626092\nvotedpp_1        0.161235410 -5.985151e-02 -5.985151e-02 -0.073626092\nvotekmt_1       -0.189400139  6.020974e-02  6.020974e-02  0.131863941\n                         KMT         DPP          npp      noparty\nDistrict         0.004964955  0.04932053  0.038673290 -0.095536878\nSex              0.023378156 -0.02778868 -0.028551061  0.029253749\nAge              0.101575567 -0.02520790 -0.147672998 -0.007394539\nEdu              0.042334075 -0.03309115  0.136058914 -0.076110629\nArear           -0.102977895  0.06402170 -0.046740492  0.070134468\nCareer          -0.089356944  0.03102496 -0.086245758  0.075142698\nCareer8         -0.072216841  0.01802044 -0.045520948  0.072794602\nEthnic           0.232873412 -0.22480062 -0.044386341  0.032863253\nParty           -0.552932225 -0.40797950  0.221691068  0.909678331\nPartyID         -0.502626783 -0.44412017  0.120607426  0.960130311\nTondu           -0.290695645  0.23811315  0.025439898  0.033897735\nTondu3          -0.238713732  0.12695577  0.015137598  0.111658730\nnI2              0.011233494 -0.10945410 -0.049676565  0.140255240\nvotetsai        -0.693651608  0.61726436  0.100547614  0.035134110\ngreen           -0.548253794  0.93939355  0.187160287 -0.531360947\nvotetsai_nm     -0.693651608  0.61726436  0.100547614  0.035134110\nvotetsai_all    -0.693651608  0.61726436  0.100547614  0.035134110\nIndependence    -0.310956352  0.38021995  0.065675949 -0.122747554\nUnification      0.261729319 -0.19958132 -0.068035040 -0.025623222\nsq               0.131389878 -0.19392358 -0.004420911  0.076464748\nTaiwanese       -0.412842831  0.38394090  0.046660623 -0.014932034\nedu              0.042334075 -0.03309115  0.136058914 -0.076110629\nfemale           0.023378156 -0.02778868 -0.028551061  0.029253749\nwhitecollar      0.090282906 -0.02748894  0.064388074 -0.078923495\nlowincome       -0.204298277  0.13578051  0.051583644  0.012272410\nincome           0.098865653 -0.00898527  0.091960553 -0.127994474\nincome_nm        0.098865653 -0.00898527  0.091960553 -0.127994474\nage              0.098304110 -0.03434278 -0.149770088  0.009835982\nKMT              1.000000000 -0.51502608 -0.102611338 -0.355122286\nDPP             -0.515026076  1.00000000 -0.144229675 -0.499157045\nnpp             -0.102611338 -0.14422968  1.000000000 -0.099449667\nnoparty         -0.355122286 -0.49915704 -0.099449667  1.000000000\npfp             -0.093447402 -0.13134892 -0.026169332 -0.090568092\nSouth           -0.116955623  0.08378116  0.001963180  0.048165592\nnorth            0.116531673 -0.07254659  0.006253082 -0.062307711\nMinnan_father   -0.253904162  0.19861056  0.041948685  0.015740006\nMainland_father  0.256598316 -0.21705341 -0.040782420 -0.011698353\nEcon_worse      -0.179121920  0.20024344  0.052639034 -0.041152032\nInequality      -0.037658703  0.09772356  0.039758547 -0.073516808\ninequality5     -0.149005409  0.13586808  0.067546777 -0.017508315\neconworse5      -0.204166799  0.21539678  0.048305247 -0.039135628\nGovt_for_public  0.239634928 -0.18789120 -0.014427996 -0.032751479\npubwelf5         0.239734391 -0.20189508 -0.034700932 -0.008379891\nGovt_dont_care  -0.104420944  0.14701198  0.028236946 -0.077308522\nhighincome       0.068551952 -0.01361176  0.084936526 -0.087998168\nvotekmt          0.742795867 -0.52158744 -0.107246193 -0.106748053\nvotekmt_nm       0.742795867 -0.52158744 -0.107246193 -0.106748053\nBlue                      NA          NA           NA           NA\nGreen                     NA          NA           NA           NA\nNo_Party                  NA          NA           NA           NA\nvoteblue         0.693651608 -0.61726436 -0.100547614 -0.035134110\nvoteblue_nm      0.693651608 -0.61726436 -0.100547614 -0.035134110\nvotedpp_1       -0.693651608  0.61726436  0.100547614  0.035134110\nvotekmt_1        0.742795867 -0.52158744 -0.107246193 -0.106748053\n                          pfp       South        north Minnan_father\nDistrict         5.233196e-02  0.08817534 -0.024908141  0.1267533871\nSex             -2.600124e-02 -0.01369798  0.019583716  0.0303055566\nAge             -4.898440e-02  0.02393014 -0.035396152 -0.1052183347\nEdu              5.617341e-02 -0.11457584  0.141301780  0.0426442841\nArear           -5.762812e-02  0.78682275 -0.887498996  0.1067372278\nCareer           3.657029e-02  0.12641321 -0.144352119 -0.0071839613\nCareer8         -4.325932e-03  0.09109309 -0.116226102  0.0346310252\nEthnic          -2.058498e-03 -0.05022013  0.053189270 -0.1594600417\nParty            7.166373e-02  0.05965787 -0.070081368  0.0767379000\nPartyID          1.462711e-02  0.05592700 -0.067838391  0.0592744653\nTondu           -4.479392e-02  0.04939688 -0.048418845  0.1210145177\nTondu3          -4.966791e-02  0.03778095 -0.043425545  0.0617932723\nnI2             -1.973447e-02  0.03158814 -0.040388590 -0.0110870255\nvotetsai        -1.807303e-01  0.12126791 -0.109942371  0.2303927079\ngreen           -1.398231e-01  0.08607322 -0.071930191  0.2143804080\nvotetsai_nm     -1.807303e-01  0.12126791 -0.109942371  0.2303927079\nvotetsai_all    -1.807303e-01  0.12126791 -0.109942371  0.2303927079\nIndependence    -5.276899e-02  0.03971729 -0.026412530  0.1702537723\nUnification      2.736177e-02  0.01257362 -0.020048014 -0.1483364182\nsq               3.921413e-02 -0.05587917  0.053268690 -0.0420846888\nTaiwanese       -3.656111e-02  0.08969998 -0.079849683  0.2199808533\nedu              5.617341e-02 -0.11457584  0.141301780  0.0426442841\nfemale          -2.600124e-02 -0.01369798  0.019583716  0.0303055566\nwhitecollar     -1.849389e-02 -0.12363077  0.153713626  0.0108871964\nlowincome        5.449595e-02  0.05163952 -0.039929274  0.0767935185\nincome          -1.515626e-05 -0.17636760  0.205572820 -0.0229896994\nincome_nm       -1.515626e-05 -0.17636760  0.205572820 -0.0229896994\nage             -5.903742e-02  0.02560138 -0.039182299 -0.1042824520\nKMT             -9.344740e-02 -0.11695562  0.116531673 -0.2539041615\nDPP             -1.313489e-01  0.08378116 -0.072546591  0.1986105563\nnpp             -2.616933e-02  0.00196318  0.006253082  0.0419486846\nnoparty         -9.056809e-02  0.04816559 -0.062307711  0.0157400056\npfp              1.000000e+00 -0.06841137  0.063354271 -0.0008731969\nSouth           -6.841137e-02  1.00000000 -0.952592201  0.2517529390\nnorth            6.335427e-02 -0.95259220  1.000000000 -0.2270190874\nMinnan_father   -8.731969e-04  0.25175294 -0.227019087  1.0000000000\nMainland_father  2.585925e-02 -0.09907812  0.109671645 -0.5615248347\nEcon_worse      -8.134896e-02  0.10923794 -0.112417166  0.0752606351\nInequality      -1.934532e-02  0.04881742 -0.026529532  0.1352759170\ninequality5     -2.507335e-02  0.05674491 -0.042459319  0.1391386069\neconworse5      -5.757065e-02  0.10003979 -0.110148886  0.1112547131\nGovt_for_public  3.659372e-02 -0.11630704  0.102869800 -0.0510941237\npubwelf5         3.615888e-02 -0.12155681  0.100825166 -0.0642469915\nGovt_dont_care  -1.135610e-02  0.06056923 -0.055057814  0.0041381106\nhighincome      -8.336403e-03 -0.16448309  0.187702447 -0.0134941809\nvotekmt         -4.299952e-02 -0.12193694  0.102781818 -0.2246849563\nvotekmt_nm      -4.299952e-02 -0.12193694  0.102781818 -0.2246849563\nBlue                       NA          NA           NA            NA\nGreen                      NA          NA           NA            NA\nNo_Party                   NA          NA           NA            NA\nvoteblue         1.807303e-01 -0.12126791  0.109942371 -0.2303927079\nvoteblue_nm      1.807303e-01 -0.12126791  0.109942371 -0.2303927079\nvotedpp_1       -1.807303e-01  0.12126791 -0.109942371  0.2303927079\nvotekmt_1       -4.299952e-02 -0.12193694  0.102781818 -0.2246849563\n                Mainland_father   Econ_worse   Inequality  inequality5\nDistrict           0.0612376491  0.014179000  0.030149861 -0.044418795\nSex               -0.0430015271 -0.040161079 -0.024190076 -0.003215767\nAge                0.1099224237  0.138700045 -0.072435195  0.025421848\nEdu                0.0724075807 -0.082016763  0.091988745 -0.058892962\nArear             -0.0922562503  0.086029166 -0.012436543  0.016338090\nCareer            -0.0801591013  0.060368030 -0.072627803  0.035989471\nCareer8           -0.1241845969  0.016503204 -0.062714339  0.011651886\nEthnic             0.1011468660 -0.069575524 -0.123512586 -0.127443080\nParty             -0.0659678024  0.003855391 -0.042579417  0.032606787\nPartyID           -0.0521323068 -0.009060957 -0.058385041  0.016194063\nTondu             -0.1548968048  0.025026417 -0.098791040  0.043564681\nTondu3            -0.1219093814 -0.003461591 -0.173966265 -0.037769174\nnI2                0.0007537522 -0.109768981 -0.138319759 -0.107188724\nvotetsai          -0.2882629473  0.210764935  0.079070165  0.183739188\ngreen             -0.2307628596  0.220838152  0.111419403  0.160953877\nvotetsai_nm       -0.2882629473  0.210764935  0.079070165  0.183739188\nvotetsai_all      -0.2882629473  0.210764935  0.079070165  0.183739188\nIndependence      -0.1675857444  0.072974169  0.079811553  0.145815000\nUnification        0.1553991720 -0.040556551 -0.027030552 -0.082560026\nsq                 0.0569424030 -0.024990316  0.022875438 -0.037148037\nTaiwanese         -0.2338665873  0.136958430  0.108759602  0.167291859\nedu                0.0724075807 -0.082016763  0.091988745 -0.058892962\nfemale            -0.0430015271 -0.040161079 -0.024190076 -0.003215767\nwhitecollar        0.0839684755 -0.083327852  0.071532615 -0.047192914\nlowincome         -0.0424105434  0.199604752  0.344774951  0.454749977\nincome             0.0196161161 -0.089232232  0.044074361 -0.052246077\nincome_nm          0.0196161161 -0.089232232  0.044074361 -0.052246077\nage                0.1087717211  0.118827133 -0.085273618  0.010173964\nKMT                0.2565983157 -0.179121920 -0.037658703 -0.149005409\nDPP               -0.2170534137  0.200243440  0.097723560  0.135868078\nnpp               -0.0407824204  0.052639034  0.039758547  0.067546777\nnoparty           -0.0116983526 -0.041152032 -0.073516808 -0.017508315\npfp                0.0258592498 -0.081348959 -0.019345321 -0.025073350\nSouth             -0.0990781196  0.109237942  0.048817418  0.056744908\nnorth              0.1096716453 -0.112417166 -0.026529532 -0.042459319\nMinnan_father     -0.5615248347  0.075260635  0.135275917  0.139138607\nMainland_father    1.0000000000 -0.032144324 -0.026333684 -0.062405488\nEcon_worse        -0.0321443237  1.000000000  0.122912977  0.200090265\nInequality        -0.0263336844  0.122912977  1.000000000  0.739307200\ninequality5       -0.0624054883  0.200090265  0.739307200  1.000000000\neconworse5        -0.0484591220  0.864383682  0.128245484  0.225695690\nGovt_for_public    0.0887656846 -0.210950973  0.006469923 -0.152518033\npubwelf5           0.0876041354 -0.243058185 -0.055075098 -0.199060022\nGovt_dont_care    -0.0176733935  0.151230095  0.125202955  0.171691541\nhighincome         0.0058193416 -0.074500849  0.041242152 -0.042346095\nvotekmt            0.2778018420 -0.192417521 -0.083358987 -0.194645949\nvotekmt_nm         0.2778018420 -0.192417521 -0.083358987 -0.194645949\nBlue                         NA           NA           NA           NA\nGreen                        NA           NA           NA           NA\nNo_Party                     NA           NA           NA           NA\nvoteblue           0.2882629473 -0.210764935 -0.079070165 -0.183739188\nvoteblue_nm        0.2882629473 -0.210764935 -0.079070165 -0.183739188\nvotedpp_1         -0.2882629473  0.210764935  0.079070165  0.183739188\nvotekmt_1          0.2778018420 -0.192417521 -0.083358987 -0.194645949\n                  econworse5 Govt_for_public     pubwelf5 Govt_dont_care\nDistrict         0.013644567     0.082745779  0.072212395    0.007171382\nSex             -0.032804610    -0.063703082 -0.060647190   -0.051134736\nAge              0.164281203    -0.106673384 -0.085697384    0.026561429\nEdu             -0.106715896     0.203214122  0.180770579   -0.102478747\nArear            0.085488887    -0.103383844 -0.085585239    0.040309698\nCareer           0.065586136    -0.167024482 -0.164647847    0.091784315\nCareer8          0.014490576    -0.108271620 -0.108424302    0.049153860\nEthnic          -0.064752162     0.077510924  0.120143125   -0.116517064\nParty            0.011069581    -0.069992438 -0.055211946   -0.033686271\nPartyID         -0.002443973    -0.064198678 -0.045930108   -0.050831034\nTondu            0.053335323    -0.135851860 -0.129476214   -0.015557021\nTondu3           0.030877095    -0.113836118 -0.090519207   -0.065789761\nnI2             -0.110647838    -0.080764446 -0.038347036   -0.044344108\nvotetsai         0.215267580    -0.235050357 -0.244455509    0.122309402\ngreen            0.234458129    -0.192385663 -0.214312038    0.160606802\nvotetsai_nm      0.215267580    -0.235050357 -0.244455509    0.122309402\nvotetsai_all     0.215267580    -0.235050357 -0.244455509    0.122309402\nIndependence     0.088249979    -0.095295636 -0.148933259    0.090923558\nUnification     -0.052561950     0.049361977  0.044977352   -0.005375208\nsq              -0.040608894     0.079921579  0.117516408   -0.043743862\nTaiwanese        0.138833621    -0.168084934 -0.195846694    0.108643395\nedu             -0.106715896     0.203214122  0.180770579   -0.102478747\nfemale          -0.032804610    -0.063703082 -0.060647190   -0.051134736\nwhitecollar     -0.101967887     0.174324139  0.176494701   -0.099728161\nlowincome        0.234252831    -0.174162591 -0.221564318    0.149055338\nincome          -0.103382179     0.183998311  0.159204817   -0.055111561\nincome_nm       -0.103382179     0.183998311  0.159204817   -0.055111561\nage              0.143120847    -0.114614689 -0.083563950    0.016560804\nKMT             -0.204166799     0.239634928  0.239734391   -0.104420944\nDPP              0.215396775    -0.187891197 -0.201895081    0.147011977\nnpp              0.048305247    -0.014427996 -0.034700932    0.028236946\nnoparty         -0.039135628    -0.032751479 -0.008379891   -0.077308522\npfp             -0.057570649     0.036593718  0.036158885   -0.011356101\nSouth            0.100039786    -0.116307041 -0.121556814    0.060569232\nnorth           -0.110148886     0.102869800  0.100825166   -0.055057814\nMinnan_father    0.111254713    -0.051094124 -0.064246991    0.004138111\nMainland_father -0.048459122     0.088765685  0.087604135   -0.017673393\nEcon_worse       0.864383682    -0.210950973 -0.243058185    0.151230095\nInequality       0.128245484     0.006469923 -0.055075098    0.125202955\ninequality5      0.225695690    -0.152518033 -0.199060022    0.171691541\neconworse5       1.000000000    -0.243285696 -0.285022540    0.143904386\nGovt_for_public -0.243285696     1.000000000  0.924168948   -0.268091064\npubwelf5        -0.285022540     0.924168948  1.000000000   -0.335138444\nGovt_dont_care   0.143904386    -0.268091064 -0.335138444    1.000000000\nhighincome      -0.090950900     0.153683496  0.135922941   -0.058868710\nvotekmt         -0.202853460     0.237434744  0.256180575   -0.138749370\nvotekmt_nm      -0.202853460     0.237434744  0.256180575   -0.138749370\nBlue                      NA              NA           NA             NA\nGreen                     NA              NA           NA             NA\nNo_Party                  NA              NA           NA             NA\nvoteblue        -0.215267580     0.235050357  0.244455509   -0.122309402\nvoteblue_nm     -0.215267580     0.235050357  0.244455509   -0.122309402\nvotedpp_1        0.215267580    -0.235050357 -0.244455509    0.122309402\nvotekmt_1       -0.202853460     0.237434744  0.256180575   -0.138749370\n                  highincome      votekmt   votekmt_nm Blue Green No_Party\nDistrict         0.067968220 -0.006810796 -0.006810796   NA    NA       NA\nSex             -0.056857167 -0.002641588 -0.002641588   NA    NA       NA\nAge             -0.192723765  0.138884171  0.138884171   NA    NA       NA\nEdu              0.353330621  0.018683420  0.018683420   NA    NA       NA\nArear           -0.204525735 -0.054636477 -0.054636477   NA    NA       NA\nCareer          -0.304313977 -0.095544354 -0.095544354   NA    NA       NA\nCareer8         -0.262535951 -0.097376587 -0.097376587   NA    NA       NA\nEthnic          -0.061122186  0.236953347  0.236953347   NA    NA       NA\nParty           -0.063318640 -0.282553626 -0.282553626   NA    NA       NA\nPartyID         -0.073711697 -0.229197708 -0.229197708   NA    NA       NA\nTondu           -0.114265880 -0.322933981 -0.322933981   NA    NA       NA\nTondu3          -0.129655871 -0.259577752 -0.259577752   NA    NA       NA\nnI2             -0.097294985  0.003098877  0.003098877   NA    NA       NA\nvotetsai        -0.041899216 -0.792565720 -0.792565720   NA    NA       NA\ngreen            0.019340121 -0.556450705 -0.556450705   NA    NA       NA\nvotetsai_nm     -0.041899216 -0.792565720 -0.792565720   NA    NA       NA\nvotetsai_all    -0.041899216 -0.792565720 -0.792565720   NA    NA       NA\nIndependence     0.023516955 -0.358229406 -0.358229406   NA    NA       NA\nUnification      0.019679378  0.258791921  0.258791921   NA    NA       NA\nsq               0.014328230  0.179341762  0.179341762   NA    NA       NA\nTaiwanese       -0.037169726 -0.460606507 -0.460606507   NA    NA       NA\nedu              0.353330621  0.018683420  0.018683420   NA    NA       NA\nfemale          -0.056857167 -0.002641588 -0.002641588   NA    NA       NA\nwhitecollar      0.303090496  0.088505426  0.088505426   NA    NA       NA\nlowincome       -0.049634472 -0.189400139 -0.189400139   NA    NA       NA\nincome           0.863015646  0.060209736  0.060209736   NA    NA       NA\nincome_nm        0.863015646  0.060209736  0.060209736   NA    NA       NA\nage             -0.218356271  0.131863941  0.131863941   NA    NA       NA\nKMT              0.068551952  0.742795867  0.742795867   NA    NA       NA\nDPP             -0.013611761 -0.521587437 -0.521587437   NA    NA       NA\nnpp              0.084936526 -0.107246193 -0.107246193   NA    NA       NA\nnoparty         -0.087998168 -0.106748053 -0.106748053   NA    NA       NA\npfp             -0.008336403 -0.042999517 -0.042999517   NA    NA       NA\nSouth           -0.164483087 -0.121936940 -0.121936940   NA    NA       NA\nnorth            0.187702447  0.102781818  0.102781818   NA    NA       NA\nMinnan_father   -0.013494181 -0.224684956 -0.224684956   NA    NA       NA\nMainland_father  0.005819342  0.277801842  0.277801842   NA    NA       NA\nEcon_worse      -0.074500849 -0.192417521 -0.192417521   NA    NA       NA\nInequality       0.041242152 -0.083358987 -0.083358987   NA    NA       NA\ninequality5     -0.042346095 -0.194645949 -0.194645949   NA    NA       NA\neconworse5      -0.090950900 -0.202853460 -0.202853460   NA    NA       NA\nGovt_for_public  0.153683496  0.237434744  0.237434744   NA    NA       NA\npubwelf5         0.135922941  0.256180575  0.256180575   NA    NA       NA\nGovt_dont_care  -0.058868710 -0.138749370 -0.138749370   NA    NA       NA\nhighincome       1.000000000  0.012203745  0.012203745   NA    NA       NA\nvotekmt          0.012203745  1.000000000  1.000000000   NA    NA       NA\nvotekmt_nm       0.012203745  1.000000000  1.000000000   NA    NA       NA\nBlue                      NA           NA           NA    1    NA       NA\nGreen                     NA           NA           NA   NA     1       NA\nNo_Party                  NA           NA           NA   NA    NA        1\nvoteblue         0.041899216  0.792565720  0.792565720   NA    NA       NA\nvoteblue_nm      0.041899216  0.792565720  0.792565720   NA    NA       NA\nvotedpp_1       -0.041899216 -0.792565720 -0.792565720   NA    NA       NA\nvotekmt_1        0.012203745  1.000000000  1.000000000   NA    NA       NA\n                    voteblue  voteblue_nm    votedpp_1    votekmt_1\nDistrict        -0.005239100 -0.005239100  0.005239100 -0.006810796\nSex             -0.001465887 -0.001465887  0.001465887 -0.002641588\nAge              0.080259089  0.080259089 -0.080259089  0.138884171\nEdu              0.050507976  0.050507976 -0.050507976  0.018683420\nArear           -0.077362920 -0.077362920  0.077362920 -0.054636477\nCareer          -0.061016958 -0.061016958  0.061016958 -0.095544354\nCareer8         -0.064384520 -0.064384520  0.064384520 -0.097376587\nEthnic           0.240392074  0.240392074 -0.240392074  0.236953347\nParty           -0.167245602 -0.167245602  0.167245602 -0.282553626\nPartyID         -0.131063568 -0.131063568  0.131063568 -0.229197708\nTondu           -0.343105947 -0.343105947  0.343105947 -0.322933981\nTondu3          -0.281938534 -0.281938534  0.281938534 -0.259577752\nnI2              0.034691345  0.034691345 -0.034691345  0.003098877\nvotetsai        -1.000000000 -1.000000000  1.000000000 -0.792565720\ngreen           -0.646229141 -0.646229141  0.646229141 -0.556450705\nvotetsai_nm     -1.000000000 -1.000000000  1.000000000 -0.792565720\nvotetsai_all    -1.000000000 -1.000000000  1.000000000 -0.792565720\nIndependence    -0.378859644 -0.378859644  0.378859644 -0.358229406\nUnification      0.248658085  0.248658085 -0.248658085  0.258791921\nsq               0.212241889  0.212241889 -0.212241889  0.179341762\nTaiwanese       -0.467994615 -0.467994615  0.467994615 -0.460606507\nedu              0.050507976  0.050507976 -0.050507976  0.018683420\nfemale          -0.001465887 -0.001465887  0.001465887 -0.002641588\nwhitecollar      0.065441080  0.065441080 -0.065441080  0.088505426\nlowincome       -0.161235410 -0.161235410  0.161235410 -0.189400139\nincome           0.059851512  0.059851512 -0.059851512  0.060209736\nincome_nm        0.059851512  0.059851512 -0.059851512  0.060209736\nage              0.073626092  0.073626092 -0.073626092  0.131863941\nKMT              0.693651608  0.693651608 -0.693651608  0.742795867\nDPP             -0.617264356 -0.617264356  0.617264356 -0.521587437\nnpp             -0.100547614 -0.100547614  0.100547614 -0.107246193\nnoparty         -0.035134110 -0.035134110  0.035134110 -0.106748053\npfp              0.180730318  0.180730318 -0.180730318 -0.042999517\nSouth           -0.121267906 -0.121267906  0.121267906 -0.121936940\nnorth            0.109942371  0.109942371 -0.109942371  0.102781818\nMinnan_father   -0.230392708 -0.230392708  0.230392708 -0.224684956\nMainland_father  0.288262947  0.288262947 -0.288262947  0.277801842\nEcon_worse      -0.210764935 -0.210764935  0.210764935 -0.192417521\nInequality      -0.079070165 -0.079070165  0.079070165 -0.083358987\ninequality5     -0.183739188 -0.183739188  0.183739188 -0.194645949\neconworse5      -0.215267580 -0.215267580  0.215267580 -0.202853460\nGovt_for_public  0.235050357  0.235050357 -0.235050357  0.237434744\npubwelf5         0.244455509  0.244455509 -0.244455509  0.256180575\nGovt_dont_care  -0.122309402 -0.122309402  0.122309402 -0.138749370\nhighincome       0.041899216  0.041899216 -0.041899216  0.012203745\nvotekmt          0.792565720  0.792565720 -0.792565720  1.000000000\nvotekmt_nm       0.792565720  0.792565720 -0.792565720  1.000000000\nBlue                      NA           NA           NA           NA\nGreen                     NA           NA           NA           NA\nNo_Party                  NA           NA           NA           NA\nvoteblue         1.000000000  1.000000000 -1.000000000  0.792565720\nvoteblue_nm      1.000000000  1.000000000 -1.000000000  0.792565720\nvotedpp_1       -1.000000000 -1.000000000  1.000000000 -0.792565720\nvotekmt_1        0.792565720  0.792565720 -0.792565720  1.000000000\n\n\n\n\nmodel &lt;- lm(Tondu ~ female + DPP + age + income + edu + Taiwanese + Econ_worse, data = TEDS_2016_no_na)\nsummary(model)\n\n\nCall:\nlm(formula = Tondu ~ female + DPP + age + income + edu + Taiwanese + \n    Econ_worse, data = TEDS_2016_no_na)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4565 -1.0357 -0.0931  0.7639  5.8369 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.5696493  0.2952361  12.091  &lt; 2e-16 ***\nfemale       0.2033016  0.0847726   2.398   0.0166 *  \nDPP          0.3935885  0.0927837   4.242 2.41e-05 ***\nage         -0.0003988  0.0034532  -0.115   0.9081    \nincome      -0.0351503  0.0150094  -2.342   0.0194 *  \nedu         -0.0641217  0.0393935  -1.628   0.1039    \nTaiwanese    0.9346107  0.0962575   9.709  &lt; 2e-16 ***\nEcon_worse  -0.1535299  0.0872766  -1.759   0.0788 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.363 on 1066 degrees of freedom\nMultiple R-squared:  0.163, Adjusted R-squared:  0.1575 \nF-statistic: 29.66 on 7 and 1066 DF,  p-value: &lt; 2.2e-16\n\n\n\nplot(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI did a similar method but with “votetsai” instead.\n\nmodel_1 &lt;- lm(votetsai ~ female + DPP + age + income + edu + Taiwanese + Econ_worse, data = TEDS_2016_no_na)\nsummary(model_1)\n\n\nCall:\nlm(formula = votetsai ~ female + DPP + age + income + edu + Taiwanese + \n    Econ_worse, data = TEDS_2016_no_na)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.07529 -0.22263 -0.02834  0.22193  0.89334 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.4017681  0.0783819   5.126 3.52e-07 ***\nfemale      -0.0193977  0.0225062  -0.862  0.38895    \nDPP          0.4927302  0.0246330  20.003  &lt; 2e-16 ***\nage         -0.0023185  0.0009168  -2.529  0.01158 *  \nincome      -0.0049516  0.0039848  -1.243  0.21428    \nedu         -0.0167235  0.0104585  -1.599  0.11011    \nTaiwanese    0.2582473  0.0255553  10.105  &lt; 2e-16 ***\nEcon_worse   0.0763049  0.0231710   3.293  0.00102 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3618 on 1066 degrees of freedom\nMultiple R-squared:  0.4534,    Adjusted R-squared:  0.4499 \nF-statistic: 126.3 on 7 and 1066 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nplot(model_1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrequency table and barchart of the Tondu variable.\n\n# install.packages(\"descr\")\n\n\n\nlibrary(descr)\nfreq(TEDS_2016_no_na$Tondu)\n\n\n\n\n\n\n\n\nPosition on unification and independence \n      Frequency Percent\n1            18   1.676\n2           131  12.197\n3           351  32.682\n4           200  18.622\n5           270  25.140\n6            73   6.797\n9            31   2.886\nTotal      1074 100.000\n\n\n\n# note: i had to change \"as.numeric\" to \"factor\" and change some of the quotes as they were the\n# wrong type and caused errors.\nTEDS_2016_no_na$Tondu&lt;-factor(TEDS_2016_no_na$Tondu,labels=c(\"Unification now\",\n\"Status quo, unif. in future\", \"Status quo, decide later\", \"Status quo\nforever\", \"Status quo, indep. in future\", \"Independence now\", \"No response\"))\nlibrary(ggplot2)\nggplot(TEDS_2016_no_na, aes(Tondu)) + \n  geom_bar()"
  },
  {
    "objectID": "Lab02.html",
    "href": "Lab02.html",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\n# creates a version of matrix A without rows 1 and 3 in this case (oposite of c())\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n# the following code shows that the \"Auto=read.csv\" method is slightly faster that the others.\n\n# Try using this simple method\ntime1 = proc.time()\nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\nproc.time()-time1\n\n   user  system elapsed \n    0.0     0.0     0.1 \n\ntime2 = proc.time()\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nproc.time()-time2\n\n   user  system elapsed \n   0.00    0.02    0.08 \n\ntime3 = proc.time()\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\")\nproc.time()-time3\n\n   user  system elapsed \n    0.0     0.0     0.1 \n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 392   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\nInstalling packages into 'C:/Users/owner/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\n\n\npackage 'MASS' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'MASS'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\owner\\AppData\\Local\\R\\win-library\\4.4\\00LOCK\\MASS\\libs\\x64\\MASS.dll to\nC:\\Users\\owner\\AppData\\Local\\R\\win-library\\4.4\\MASS\\libs\\x64\\MASS.dll:\nPermission denied\n\n\nWarning: restored 'MASS'\n\n\npackage 'ISLR' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\owner\\AppData\\Local\\Temp\\Rtmp0i82sH\\downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n#confidence interval is the range that should contain the true population parameter\n#prediction interval is the range that should contain the true data point value\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Lab02.html#indexing-data-using",
    "href": "Lab02.html#indexing-data-using",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "A=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\n# creates a version of matrix A without rows 1 and 3 in this case (oposite of c())\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4"
  },
  {
    "objectID": "Lab02.html#loading-data-from-github-remote",
    "href": "Lab02.html#loading-data-from-github-remote",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n# the following code shows that the \"Auto=read.csv\" method is slightly faster that the others.\n\n# Try using this simple method\ntime1 = proc.time()\nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\nproc.time()-time1\n\n   user  system elapsed \n    0.0     0.0     0.1 \n\ntime2 = proc.time()\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nproc.time()-time2\n\n   user  system elapsed \n   0.00    0.02    0.08 \n\ntime3 = proc.time()\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\")\nproc.time()-time3\n\n   user  system elapsed \n    0.0     0.0     0.1 \n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 392   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "Lab02.html#load-data-from-islr-website",
    "href": "Lab02.html#load-data-from-islr-website",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "Lab02.html#additional-graphical-and-numerical-summaries",
    "href": "Lab02.html#additional-graphical-and-numerical-summaries",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "Lab02.html#linear-regression",
    "href": "Lab02.html#linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "ptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\nInstalling packages into 'C:/Users/owner/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\n\n\npackage 'MASS' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'MASS'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\owner\\AppData\\Local\\R\\win-library\\4.4\\00LOCK\\MASS\\libs\\x64\\MASS.dll to\nC:\\Users\\owner\\AppData\\Local\\R\\win-library\\4.4\\MASS\\libs\\x64\\MASS.dll:\nPermission denied\n\n\nWarning: restored 'MASS'\n\n\npackage 'ISLR' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\owner\\AppData\\Local\\Temp\\Rtmp0i82sH\\downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n#confidence interval is the range that should contain the true population parameter\n#prediction interval is the range that should contain the true data point value\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "Lab02.html#multiple-linear-regression",
    "href": "Lab02.html#multiple-linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "Lab02.html#non-linear-transformations-of-the-predictors",
    "href": "Lab02.html#non-linear-transformations-of-the-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Lab02.html#qualitative-predictors",
    "href": "Lab02.html#qualitative-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "href": "Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "summary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "workshop_ai_model_benchmarking.html",
    "href": "workshop_ai_model_benchmarking.html",
    "title": "Workshop AI Model Benchmarking",
    "section": "",
    "text": "My quick first impressions in class:\nAccuracy:\nAll of the AIs look to be accurate this time around, but only because they are citing articles with web access. I had to ask Claude for citations and it generated new text rather than add citation to existing text. Chat GPT does like to lie to you as I can attest to in the past, but It looks like when it is citing articles that it has to reference that is less likely, copilot looks to be citing the article accurately enough as it’s pulling live.\nBias:\nChat GPT likes to cite financial articles like the wall street journal and smaller educational articles (not academic papers or research. But meant for informing the general public about new developments and research). Claude Likes to cite academic articles and research when prompted. Copilot Likes to cite mostly educational articles with a little financial mixed in.\nDepth:\nChat gpt gave a good general overview of the current advancements in data science with a series of financial articles and educational articles, but lacked academic papers and research for a proper deep dive. Claude did go in depth and focused on academic papers and research and cited them (but I had to ask for it on the version I had access to). Copilot was the least robust description of the topic, and cited mostly educational articles so out of the bunch I would say it had the least depth.\nClarity:\nThe way each AI is formatted, I find that Chat GPT is the most clear in writing style followed by copilot, then Claude. Claude seems to repeat itself and structures it’s citations in a jarring manner, Copilot has its citations at the end and is short and to the point, and Chat GPT is closest to a conversation of the three. \nOverall, I would say that I found this exercise helpful as it shows that if one is to use AI in research, then they should be aware of the strengths and weaknesses of particular tools and AI is no different. In a workflow scenario it may be best to use something like Copilot or chat GPT with citations as a starting point to understand the gist of a topic, then move to Claude for more in depth research."
  },
  {
    "objectID": "assignment4.html",
    "href": "assignment4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Looking at the Biden-Xi summit data:\nThe data starts with a focuses on Hashtags from Twitter data about President Biden and Xi summit in Novemeber 2021. The generated chart shows some f the more prominent Hashtags and how often they appear together with thicker blue lines meaning they are more closely linked. The majority of Hashtags cluster around #xijinping, #joebiden, #biden, #america, #americans, #china, #coronavirus, and #fentanyl. These Hashtags cover a range of topics relevant to important issues at the time of the summit. there are also less bold lines indicating alternat hasttags for countries as well as a smaller cluster around #biden such as #uyghurgenocide, #tibetans, and #humanrights that would indicate that some were pressuring Biden to talk, action related to these issues during the summit, or to object to talks with Xi due to these issues.\nAnother Network plot looks at the twitter handles related to prominent accounts that include the media, participants in the summit, and other accounts that were related to tweets related to the summit. similarly to the previous plot, lines link related usernames to each other based on relevence. This plot shows that prominant users are often clustered in their interaction or mentions, such is seen with foxbusiness and morningsmaria, or politico bing linked to nahaltoosi, phelimkine, whnsc, and anderscorr.\nLooking at the US presidential inaugural speeches:\nOverall US presidential inaugural speeches was visualized using Wordcloud to better see what the most common words are. The overall picture shows that words such as “government”, “states”, “people”, “may”, “public”, “war”, “great”, “nation”, and “citizens” (just a few examples) are common across the data of all inaugural speeches.\nIn contrast, when looking at the inaugural speeches of president Bush, Biden, Obama, and Trump, we can see different words being prioritized in the speeches and compared to each other. Bush used “freedom” significantly more that any other word in his speech, with “liberty” and “free” being used significantly less but a little more than others. Trump used “america” the most and eclipsed the use of any other word in the comparison by a large margin, he also used “american”, “dreams”, “back”, “country”, “protected”, “people”, and “across” to a significant degree and showing that his speech was the most repetitive of the four. Biden used “us” the most with his other most used words being “democracy”, “can”, “story”, “one”, and “another”. Obama’s speech was the least repetitive with his word usage being more uniform with a few words being more common such as “must”, “journey”, “time”, and “generation”.\nIn the Lexical dispersion plots, we find that the term “american” has seen more use over time with some of the heaviest usage being under Trump in 2017 and Clinton in 1997 and with most usage appearing to have middle to high Relative token index scores. While not as much data was available for the term “Trade”, it is notable that under Eisenhower and Clinton it tends to have a higher index while Bush and Trump have lower index scores. The word “people” seems to have not changed significantly over time and has had a range of Relative token index scores.\nWhat is Wordfish?\nFrom the Wordfish website: “Wordfish is a computer program written in the R statistical language to extract political positions from text documents. Word frequencies are used to place documents onto a single dimension. Wordfish is a scaling technique and does not need any anchoring documents to perform the analysis. Instead, it relies on a statistical model of word counts. The current implementation assumes a Poisson distribution of word frequencies. Positions are estimated using an expectation-maximization algorithm. Confidence intervals for estimated positions can be generated from a parametric bootstrap.The name Wordfish pays tribute to the French meaning of the word “poisson”.”\nIn essence it is a tool that can be used with Quanteda that can be used to look at text.\nFrom the Quanteda website: “Wordfish is a Poisson scaling model of one-dimensional document positions (Slapin and Proksch 2008). Wordfish also allows for scaling documents, but in comparison to Wordscores, reference scores/texts are not required. Wordfish is an unsupervised one-dimensional text scaling method, meaning that it estimates the positions of documents solely based on the observed word frequencies.”\nHow to compare positions (hint: wordfish and scaling methods)\nIt would look like higher frequency usage of a term would result in documents have higher scores on the Relative token index.\nCreate a corpus using government documents selected from the govinfo.gov website (use sample program govtdata01.R)\nsaved as a pdf but it’s so big that github refuses to let me upload it to my site (its a 876 page PDF of HD pictures of gov documents at 414,820 KB), as such I can upload it here, but i did create a corpus.\nSmall update, but the due date was extended and a new version of the code was given in class that fixed my issue as it now downloads seperate pdf files rather than one big one, I ran it and was downloading them until Dr Ho told me to stop (code auto downloads 1000 government documents, so he didn’t want people overloading the government website with download requests)."
  },
  {
    "objectID": "prepare_for_class_8.html",
    "href": "prepare_for_class_8.html",
    "title": "Prepare for class 8",
    "section": "",
    "text": "The instructions indicated I was expected to use an AI model(s) for this work, so Grok was used to help with this task.\nSummary of the readings:\nA Survey of Large Language Models (ResearchGate, 2023) Overall this article highlights how LLMs built on Transformer architectures have evolved from traditional statistical and neural language models, to pre-trained models capable of solving complex NLP tasks. Knowledge Mining benefits from LLMs’ ability to extract structured information from unstructured text via pre-training on massive corpora, adaptation tuning (e.g., fine-tuning), and advanced utilization techniques (e.g., prompting). The paper emphasizes LLMs’ capacity to uncover latent knowledge patterns, such as factual extraction and reasoning, due to their scale and pre-training. Specific takeaways for Knowledge Mining are that LLMs enhance it by enabling semantic understanding, entity recognition, and relationship extraction at scale. Techniques like retrieval-augmented generation (RAG) further improve knowledge extraction by combining LLMs with external knowledge bases.\nA Survey on Evaluation of Large Language Models (ACM, 2024) Overall this article reviews evaluation methods for LLMs, focusing on dimensions like accuracy, robustness, and fairness in tasks such as question answering (QA) and code generation. It notes that LLMs like InstructGPT and ChatGPT outperform earlier models in extracting and synthesizing knowledge from diverse domains (e.g., social sciences, engineering). However, challenges like hallucination and limited general science performance indicate gaps in reliable knowledge mining. Specific takeaways for Knowledge Mining include the ability to evaluate LLMs on task-specific benchmarks (e.g., QA accuracy) shows their growing role in mining actionable insights. and robustness in out-of-distribution (OOD) scenarios remains a hurdle, impacting the reliability of mined knowledge.\nRecent Advances in Natural Language Processing via Large Pre-Trained Language Models (ResearchGate, 2021) Overall this article details how pre-trained language models (PLMs) like BERT and GPT have revolutionized NLP by leveraging unsupervised pre-training and fine-tuning. For Knowledge Mining, PLMs excel in tasks like closed-book QA, event extraction, and joint entity-relation extraction, reducing reliance on hand-crafted features. The paper highlights the shift from task-specific models to general-purpose models adaptable to mining tasks. A specific takeaway for Knowledge Mining is that PLMs provide a foundation for extracting structured knowledge (e.g., entities, relations) from raw text, with fine-tuning enabling domain-specific mining (e.g., biomedical, legal). Their scalability and adaptability mark a significant leap from earlier NLP approaches.\nKey Concepts & terms:\nPre-Training and Scale: LLMs leverage vast corpora and large parameter sizes (e.g., tens/hundreds of billions) to encode rich knowledge, enabling deeper mining of semantic and contextual information.\nAdaptation Techniques: Fine-tuning and prompting allow LLMs to specialize in Knowledge Mining tasks like entity extraction, relation detection, and QA, tailoring general knowledge to specific domains.\nTask Versatility: LLMs handle diverse mining tasks (e.g., QA, event extraction, sentiment analysis), shifting Knowledge Mining from narrow, rule-based systems to broad, data-driven approaches.\nEvaluation Metrics: Accuracy, robustness, and fairness metrics assess LLMs’ reliability in mining trustworthy knowledge, though issues like hallucination persist.\nTransformer Architecture: The backbone of LLMs, Transformers enable parallel processing and attention mechanisms critical for understanding complex text structures in Knowledge Mining.\nFrom Rule-Based to Data-Driven: Early NLP relied on statistical models and manual rules for Knowledge Mining (e.g., keyword matching). LLMs shift this to unsupervised learning, extracting knowledge from patterns in massive datasets.\nSemantic Understanding: LLMs capture context and meaning beyond surface-level text, improving entity recognition, relation extraction, and factual synthesis.\nScalability: Larger models (e.g., GPT-3, ChatGPT) mine knowledge from diverse, unstructured sources at unprecedented scales, surpassing smaller PLMs like BERT.\nDomain Adaptation: Fine-tuning and prompting make LLMs adaptable to specialized fields (e.g., medicine, engineering), enhancing domain-specific knowledge extraction.\n\n\n\nknowledge_map\n\n\nPrepare for discussion:\n\nHow NLP and LLM can assist in your research and provide data/assistance for your project? Give specific examples.\nNLP and LLM could be used to better process raw data such as social media posts, internet searches, or other text based data in order to be used in my research on finding correlations between negative comments relevant to inflation. This serves to both clean the data by filtering out the majority of unrelated information, and to reorganize the relevant data into categories such as by location, time, date, and by good (match it up with CPI goods)\nName limitations and suggest solutions:\nAccess to relevant data could be an issue as many social media sites have moved to restrict 3rd party access in recent years. Solution would be to limit analysis to more accessible forms such as google analytics of search terms and results.\nAnother limitation could be the NLP and LLM used not being up to date on communication such as relatively new slang or common terms. A possible solution would be to use updated NLP and LLM models or data augmentation or retraining with new data in order to bring them up to speed to current language usage.\nWrite an AI for research guide (you can use bullet points) incorporating ethnical, technical and academic concerns that could lead to more problems than solutions (e.g. Hinton)\nWhile AI is a tool that can help research, there are a number of factors to consider when one does utilize such a tool: AI is an imperfect tool that often has issues with inaccurate information from unreliable sources or “hallucinations” (Where the AI will 100% lie to your face), bias, robustness, and inability to follow stated directions. As such, one should always double check the output from such AI models rather than blindly trusting in it.\n\nNot all AI models are created equal, and as such it is advised to select the right model for the task at hand based on its relative strengths and weaknesses.\nWhen using data to train AI or harvested by AI for research purposes (or in general), there are a great deal of ethical concerns related to what data is used. While it may be easy to understand that using sensitive information such as medical, trade secrets, or otherwise confidential information can have some issues, It is also important to consider the ethical ramifications of using work and data of others that might wish to be excluded from such activities.\nMany have expressed concern (and often anger) about their work and posts being used for AI training data and in research data in general without permission (and sometimes used for commercial purposes) and in some cases without anonymising data.\nIn addition to ethical concerns there are also possible legal ramifications as a number of ongoing lawsuits have been filed.\nIt is also important to disclose such AI usage in general, but especially academic settings and avoid passing the work done as one’s similarly to when one cites sources."
  },
  {
    "objectID": "prepare_for_class_9.html",
    "href": "prepare_for_class_9.html",
    "title": "Prepare for class 9",
    "section": "",
    "text": "March Madness (NCAA Men’s Basketball DI)\n\nIf you build a predictive model, how would you plan for such a project?\n\nI would start by getting data from NCAA Men’s Basketball DI in years past as well as performance data or stats of players.\n\nHow to get data?\n\nThis information should be available on a number of websites that track this kind of information for fantasy basketball or sports as well as on official channels and sports data archives.\n\nWhat is the dependent variable? Models?\n\nThe dependent variable would be the results or rankings of the The NCAA college basketball single-elimination tournament could also be the outcome of a game if one wanted an analysis on the game level instead.\n\nWhat variables can be used to predict the outcome?\n\nPrevious team performance in March Madness, player performance or stats, Points Per Game (PPG) indicate a team’s offensive strength, Defensive Efficiency measures points allowed per 100 possessions, Rebound Rate shows the percentage of missed shots retrieved by the team, Turnover Rate reflects the percentage of possessions ending in turnovers."
  }
]